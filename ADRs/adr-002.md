# Architecture Decision Record: Provider-Level Authentication with Resource Override

## Status
Rejected

## Summary

Add optional provider-level authentication while maintaining the ability to override connections at the resource level. This provides familiar patterns for existing clusters while preserving the unique ability to create clusters and deploy to them in a single apply - solving the two-phase apply problem that affects nearly every Kubernetes + Terraform user.

## Context

### The Two-Phase Apply Problem

The most painful problem in Kubernetes + Terraform today:

```hcl
# What everyone wants to do:
resource "aws_eks_cluster" "main" {
  name = "prod"
}

resource "kubernetes_deployment" "app" {
  metadata {
    name = "my-app"
  }
  # This FAILS - provider can't connect to cluster that doesn't exist yet!
}
```

Current "solutions" are all terrible:
1. **Two separate Terraform states** - Complexity and state management nightmare
2. **Two-phase apply with -target** - Error-prone, breaks automation
3. **Null resources with local-exec** - Hacky, fragile, no drift detection
4. **External scripts** - Loses Terraform's declarative benefits

### The Inline Connection Solution

k8sconnect solves this elegantly:

```hcl
resource "aws_eks_cluster" "main" {
  name = "prod"
}

resource "k8sconnect_manifest" "app" {
  yaml_body = file("deployment.yaml")
  cluster_connection = {
    host = aws_eks_cluster.main.endpoint
    cluster_ca_certificate = aws_eks_cluster.main.certificate_authority[0].data
    # ... auth details ...
  }
  # This WORKS - connection evaluated at apply time!
}
```

### The Problem

The current design requires every resource to specify its cluster connection:

```hcl
resource "k8sconnect_manifest" "app1" {
  yaml_body = file("app1.yaml")
  cluster_connection = {  # Required on EVERY resource
    kubeconfig_file = "~/.kube/config"
    context = "prod"
  }
}

resource "k8sconnect_manifest" "app2" {
  yaml_body = file("app2.yaml")
  cluster_connection = {  # Same connection repeated
    kubeconfig_file = "~/.kube/config"
    context = "prod"
  }
}
```

This creates several issues:
1. **Repetitive**: Same connection copied across dozens/hundreds of resources
2. **Unfamiliar**: Deviates from Terraform provider norms
3. **Barrier to adoption**: Scares off users before they experience the benefits
4. **Migration friction**: Can't easily port configs from official provider

### Why This Matters

The inline authentication isn't just about multi-cluster - it solves the **two-phase apply problem** that plagues every Kubernetes Terraform user:

```hcl
# The nightmare with official provider:
# Phase 1: Create cluster
resource "aws_eks_cluster" "main" {
  name = "prod"
}

# Phase 2: Deploy to it (CAN'T DO IN SAME APPLY!)
resource "kubernetes_deployment" "app" {
  # This fails because provider can't connect to 
  # not-yet-existing cluster
}
```

This forces users into terrible patterns:
- Maintaining two separate Terraform states
- Running `terraform apply` twice with different targets
- Complex CI/CD pipelines with multiple stages
- Manual intervention between infrastructure and application

**This problem affects nearly EVERY Kubernetes + Terraform user**, not just multi-cluster scenarios.

However, requiring inline connection on every resource creates adoption friction:

```hcl
# Even for existing clusters, this feels repetitive:
resource "k8sconnect_manifest" "app1" {
  yaml_body = file("app1.yaml")
  cluster_connection = { ... }  # Same connection
}

resource "k8sconnect_manifest" "app2" {
  yaml_body = file("app2.yaml")
  cluster_connection = { ... }  # Repeated again
}

# 50 more resources...
```

Users see the unfamiliar pattern and don't realize it solves their two-phase problem. They just see "different" and "complex" and use the official provider instead - then suffer with two-phase applies.

### Current State of Terraform Kubernetes Providers

- **Official provider**: Provider-level auth only, requires two-phase apply
- **kubectl provider**: Provider-level auth only, requires two-phase apply
- **kubernetes-alpha** (deprecated): Required inline config, confused users
- **k8sconnect**: Currently requires inline config, limiting adoption

### Real-World Problems This Solves

#### Problem 1: Cluster + Apps in One Apply
```hcl
# With official provider: IMPOSSIBLE in one apply
# With k8sconnect: Just works!
resource "google_container_cluster" "primary" {
  name = "prod"
}

resource "k8sconnect_manifest" "ingress_controller" {
  yaml_body = file("nginx-ingress.yaml")
  cluster_connection = {
    host  = google_container_cluster.primary.endpoint
    # ... auth details ...
  }
}
```

#### Problem 2: Cluster Bootstrap
```hcl
# Create cluster and immediately install:
# - Ingress controller
# - Cert-manager  
# - Monitoring stack
# - GitOps operator
# ALL IN ONE APPLY!
```

#### Problem 3: Dynamic Environments
```hcl
# Spin up complete environment (cluster + apps) for PR review
# Tear down everything when PR is merged
# No multi-stage CI/CD complexity
```

## Decision

Implement a hybrid authentication model:

1. **Provider accepts connection configuration** (optional)
2. **Resources inherit provider connection by default**
3. **Resources can override with their own connection**
4. **Connection resolution follows clear precedence rules**
5. **State records the effective connection for safety**

### Design

#### Provider Schema Addition
```hcl
provider "k8sconnect" {
  # Option 1: Kubeconfig file
  kubeconfig_file = "~/.kube/config"
  context         = "prod"
  
  # Option 2: Inline credentials
  host                   = "https://cluster.example.com"
  cluster_ca_certificate = base64encode(file("ca.crt"))
  
  # Option 3: Raw kubeconfig
  kubeconfig = file("kubeconfig.yaml")
  
  # Exec auth works everywhere
  exec {
    api_version = "client.authentication.k8s.io/v1"
    command     = "aws"
    args        = ["eks", "get-token", "--cluster-name", "prod"]
  }
}
```

#### Resource Behavior
```hcl
# Default: Uses provider connection (existing clusters)
resource "k8sconnect_manifest" "app" {
  yaml_body = file("app.yaml")
  # No cluster_connection needed!
}

# Override: Deploy to just-created cluster (ONE PHASE!)
resource "aws_eks_cluster" "new" {
  name = "prod"
}

resource "k8sconnect_manifest" "bootstrap" {
  yaml_body = file("bootstrap.yaml")
  cluster_connection = {
    host                   = aws_eks_cluster.new.endpoint
    cluster_ca_certificate = aws_eks_cluster.new.certificate_authority[0].data
    # ... exec config for auth ...
  }
  # This WORKS in the same apply! No two-phase nightmare!
}
```

### Connection Resolution Algorithm

```go
func (r *manifestResource) resolveConnection(ctx context.Context) (ClusterConnectionModel, error) {
    // 1. Resource connection takes precedence
    if !r.data.ClusterConnection.IsNull() {
        return r.convertObjectToConnectionModel(ctx, r.data.ClusterConnection)
    }
    
    // 2. Fall back to provider connection
    if r.providerConfig != nil && r.providerConfig.HasConnection() {
        return r.providerConfig.Connection, nil
    }
    
    // 3. Error if no connection available
    return ClusterConnectionModel{}, fmt.Errorf(
        "no cluster connection specified: either configure the provider or set cluster_connection on the resource")
}
```

### State Management Strategy

**Critical Decision**: Resources using provider connection must record it in their state.

```go
// During Create/Update
effectiveConn := r.resolveConnection(ctx)
r.data.ClusterConnection = r.convertConnectionToObject(effectiveConn)
```

This ensures:
- Resources remember which cluster they're in
- Provider config changes don't accidentally move resources
- State is portable between configurations
- Import continues to work correctly

### Migration Safety

When provider connection changes, resources detect potential cluster switches:

```go
// During Update
oldConn := r.stateConnection
newConn := r.resolveConnection()

if r.connectionTargetsDifferentCluster(oldConn, newConn) {
    // Validate ownership annotation (same as today)
    if err := r.validateOwnership(ctx); err != nil {
        return fmt.Errorf("provider connection change would move resource to different cluster: %w", err)
    }
}
```

## Consequences

### Positive

- **Familiar pattern**: Matches every other Terraform provider
- **Solves two-phase apply**: THE killer feature - create cluster and deploy in one apply
- **Reduced repetition**: Single connection for existing clusters
- **Easy migration**: Drop-in replacement for official provider
- **Preserves inline power**: Can still create and deploy in one phase
- **Best of both worlds**: Simple for existing clusters, powerful for new ones
- **Adoption driver**: Removes friction while keeping unique capabilities

### Negative

- **Implementation complexity**: Two sources of connection config
- **State migration**: Existing resources need careful handling
- **Testing burden**: Must test all connection combinations
- **Documentation**: More scenarios to explain

### Neutral

- **Breaking change**: No - existing configs continue working (resource connection still supported)
- **Precedence rules**: Must be clearly documented
- **Connection caching**: Works identically for both patterns
- **Existing users**: Can continue using inline connections exactly as before

## Implementation Plan

### Phase 1: Provider Schema
1. Add connection fields to provider schema
2. Store provider config during Configure
3. Pass to resources via ResourceData

### Phase 2: Resource Changes  
1. Make cluster_connection optional
2. Add connection resolution logic
3. Update state management to record effective connection

### Phase 3: Safety Features
1. Detect cluster switches
2. Add validation and warnings
3. Comprehensive error messages

### Phase 4: Migration Support
1. Document migration path
2. Add examples for common scenarios
3. Consider migration tool/command

## Migration Examples

### From Official Provider
```hcl
# Before (official provider)
provider "kubernetes" {
  config_path    = "~/.kube/config"
  config_context = "prod"
}

resource "kubernetes_deployment" "app" {
  metadata {
    name = "app"
  }
  spec {
    # ... nested HCL ...
  }
}

# After (k8sconnect with provider auth)
provider "k8sconnect" {
  kubeconfig_file = "~/.kube/config"
  context         = "prod"
}

resource "k8sconnect_manifest" "app" {
  yaml_body = file("deployment.yaml")
}
```

### Gradual Resource Migration
```hcl
# Stage 1: Add provider config
provider "k8sconnect" {
  kubeconfig_file = "~/.kube/config"
  context = "prod"
}

# Stage 2: New resources use provider connection
resource "k8sconnect_manifest" "new_app" {
  yaml_body = file("new-app.yaml")
  # No cluster_connection needed
}

# Stage 3: Migrate existing resources by removing cluster_connection
# (Terraform detects no effective change)
resource "k8sconnect_manifest" "existing_app" {
  yaml_body = file("app.yaml")
  # cluster_connection = { ... }  # Removed
}
```

## Alternatives Considered

### 1. Provider Connection Only
- **Rejected**: Loses unique multi-cluster capability
- Many users specifically need per-resource connections

### 2. Deprecate Inline Connection  
- **Rejected**: Breaking change for existing users
- Removes a key differentiator

### 3. Connection Templates/Aliases
```hcl
# Considered but too complex
connection_template "prod" {
  kubeconfig_file = "~/.kube/config"
  context = "prod"
}

resource "k8sconnect_manifest" "app" {
  connection = connection_template.prod
}
```
- **Rejected**: Adds complexity without clear benefit
- Terraform doesn't support this pattern well

## Security Considerations

1. **Credential Exposure**: Provider config may be logged/displayed
   - Same risk as official provider
   - Document secure credential practices

2. **Accidental Cluster Targeting**: Changing provider could affect many resources
   - Mitigated by ownership validation
   - Clear warning messages

3. **State Sensitivity**: Connection details stored in state
   - Mark sensitive as appropriate
   - Same as current design

## References

- [Official Kubernetes Provider Configuration](https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs)
- [Terraform Provider Design Principles](https://www.terraform.io/plugin/best-practices/provider-design-principles)
- [kubectl Provider Pattern](https://registry.terraform.io/providers/gavinbunney/kubectl/latest/docs)

## Decision Outcome

Implement provider-level authentication with resource-level override capability. This solves two critical problems:

1. **Adoption friction**: Familiar provider-level auth for existing clusters
2. **Two-phase apply hell**: Inline auth enables single-apply cluster creation + deployment

No other Kubernetes provider can create infrastructure and deploy to it in one apply. This feature alone justifies the implementation complexity. Combined with accurate diffs from ADR-001, this creates a provider that is both more powerful AND easier to use than any alternative.
