# QA Checklist

## Mission: Break EVERYTHING or polish EVERYTHING

**PRIMARY GOALS:**
1. **World-Class UX** - Every error and warning must be helpful, clear, and actionable
2. **No Surprises** - Behavior must be predictable and well-communicated
3. **Enterprise Quality** - This is production software, not a prototype

DO NOT STOP until EVERY item is checked off.

---

## IMPORTANT: How to Use This Document

**DO NOT modify this file during testing.** This is a reusable template. Checking off boxes, adding notes, or editing this document in any way defeats its purpose for future releases.

**For each test run, create a single results file:**

1. Create ONE file: `QA-RESULTS-<description>.md` (e.g., `QA-RESULTS-ADR023.md`)
2. At the top, record metadata: date, provider version/commit, Terraform version, Kubernetes version
3. ALL phases go into this one file. Do not create separate files per phase or section.
4. For each phase tested, log:
   - Phase name
   - Each step attempted with **PASS** or **FAIL**
   - For FAILs: exact error output, expected vs actual behavior, severity
   - Any observations, UX concerns, or notes worth capturing
5. Update the same file as you progress through phases â€” it is the single living record of the entire test run
6. At the bottom, record a summary: total pass/fail counts, blocking issues, and overall assessment

**Process: run all phases, then fix, then certify.**

Do NOT restart from Phase 0 after every bug found â€” that wastes time re-running passing phases. Instead:

1. Run through ALL phases, collecting every bug and UX issue
2. Fix everything in one batch (with unit tests for each fix)
3. Do ONE final clean run from Phase 0 on the release commit to certify

The final clean run is the one that matters for release. Intermediate runs are for bug discovery only.

One test run = one results file. This checklist is the reusable reference.

**Test environment: `scenarios/kind-validation/`**

All testing runs against the live Kind cluster and Terraform state in `scenarios/kind-validation/`. The `main.tf`, `edge-case-tests.tf`, and supporting YAML files in that directory are the starting point. You are encouraged to stretch and modify these files as needed to push the provider to its limits. The goal of each phase is to try as hard as you can to break the provider and validate every edge case you can find. This checklist is kept intentionally loose so that each run is slightly non-deterministic for broader coverage. When gaps are found that should become permanent parts of the manual testing procedure, edit this checklist and modify the Terraform files in the folder accordingly.

Do NOT create throwaway test configs in `/tmp` or other locations. Work within the scenario directory.

---

## Phase 0: Setup and Happy Path (MUST PASS FIRST!)

### Build and Install
- [ ] Run `make install` from project root (builds and installs provider)
- [ ] Verify provider installed to `~/.terraform.d/plugins/`

### Environment Setup
- [ ] Navigate to `scenarios/kind-validation/`
- [ ] Run `./reset.sh` to reset environment
- [ ] Review `main.tf` - does it test all core behaviors and edge cases?
- [ ] Read any relevant docs for features being tested

### Initial Apply (The Happy Path)
- [ ] Run `terraform init` - succeeds without errors?
- [ ] Run `terraform plan` - output looks correct? No unexpected changes?
- [ ] Run `terraform apply` - all ~62 resources created successfully?
- [ ] Review apply output - any warnings or concerning messages?
- [ ] Check pod status: `KUBECONFIG=./kind-validation-config kubectl get pods -A`
- [ ] All pods running/completed (except intentional test failures)?

### Zero-Diff Stability Test
- [ ] Run `terraform apply` again immediately
- [ ] Should show "No changes" (zero diff)
- [ ] If diff appears, investigate before proceeding!
- [ ] Common culprits: nodePort randomness, timestamp fields, HPA replicas

### ğŸ¯ Behavioral Expectations (NO SURPRISES!)
Verify these behaviors are consistent:
- [ ] CREATE never silently adopts existing resources
- [ ] Import is always required for unmanaged resources
- [ ] Warnings appear for drift, not errors (unless fatal)
- [ ] Resource removal is always explicit (no silent deletions)
- [ ] Ownership transitions are clearly communicated
- [ ] State remains consistent after errors

**STOP HERE if Phase 0 fails. Fix issues before continuing.**

---

## Phase 1: k8sconnect_object Error Testing

**ğŸ¯ UX FOCUS**: Every error must tell the user WHAT went wrong, WHY it happened, and HOW to fix it

### Invalid Resource Discovery
- [ ] Invalid kind (completely made up)
Â Â - [ ] âœ… Error clearly states the kind doesn't exist?
Â Â - [ ] âœ… Suggests checking spelling or available kinds?
- [ ] Invalid API version (non-existent group)
Â Â - [ ] âœ… Error explains API version format?
Â Â - [ ] âœ… Shows example of correct format?
- [ ] Malformed API version (invalid format)
- [ ] CRD that doesn't exist yet
Â Â - [ ] âœ… Error suggests checking if CRD is installed?
Â Â - [ ] âœ… Provides kubectl command to list CRDs?
- [ ] Valid kind but wrong API version

### Schema Validation Errors
- [ ] Missing required field (Deployment without selector)
- [ ] Invalid field name (not in schema)
- [ ] Wrong field type (string instead of int)
- [ ] Invalid nested field
- [ ] Extra fields not in schema

### Naming and Format Validation
- [ ] Invalid resource name (uppercase)
- [ ] Invalid resource name (too long >253 chars)
- [ ] Invalid resource name (special characters)
- [ ] Invalid namespace name (uppercase)
- [ ] Invalid label key (starts with number)
- [ ] Invalid label key (invalid characters)
- [ ] Invalid annotation key format

### Value Validation
- [ ] Negative value for positive-only field (replicas: -5)
- [ ] Value out of range
- [ ] Invalid enum value
- [ ] Invalid port number (>65535)
- [ ] Invalid protocol value

### Resource Constraints
- [ ] Resource in non-existent namespace
- [ ] Namespace-scoped resource without namespace
- [ ] Cluster-scoped resource with namespace specified
- [ ] Immutable field modification
- [ ] Identity field change (triggers replacement)

### YAML Issues
- [ ] Empty yaml_body
- [ ] yaml_body with only comments
- [ ] YAML with unknown interpolations during plan
- [ ] Very large YAML payload

---

## Phase 2: k8sconnect_patch Error Testing

### Target Validation
- [ ] Patch non-existent resource
- [ ] Patch with invalid target kind
- [ ] Patch with invalid target API version
- [ ] Patch with invalid target namespace
- [ ] Patch cluster-scoped resource with namespace

### Patch Data Issues
- [ ] Invalid JSON in patch
- [ ] Empty patch
- [ ] Patch removes required field
- [ ] Patch modifies immutable field
- [ ] Strategic merge patch on resource that doesn't support it

### Conflict Scenarios
- [ ] Patch field owned by different manager
- [ ] Patch field that doesn't exist
- [ ] Patch with field path typo

---

## Phase 3: k8sconnect_wait Error Testing

### Object Reference Issues
- [ ] Wait on non-existent resource 
- [ ] Wait with invalid kind in object_ref
- [ ] Wait with invalid API version
- [ ] Wait with wrong namespace

### Condition Waits
- [ ] Wait for condition that never exists
- [ ] Wait for condition with wrong status
- [ ] Wait for condition on resource that doesn't support conditions
- [ ] Condition timeout (with good error showing current state)

### Field Waits
- [ ] Wait for non-existent field
- [ ] Wait for field with typo in path
- [ ] Wait for field that never gets populated
- [ ] Field timeout showing current value

### Field Value Waits
- [ ] Wait for impossible value
- [ ] Wait for value with wrong type
- [ ] Multiple fields, one impossible
- [ ] Field value timeout showing all current values

### Rollout Waits
- [ ] Wait for rollout on non-rollout resource (ConfigMap)
- [ ] Wait for rollout with bad image (shows pod errors)
- [ ] Wait for rollout timeout

### Timeout Scenarios
- [ ] Zero timeout
- [ ] Invalid timeout format
- [ ] Very short timeout (<1s)

---

## Phase 4: Datasource Error Testing

### data.k8sconnect_object
- [ ] Missing resource 
- [ ] Invalid kind 
- [ ] Invalid API version
- [ ] Invalid namespace
- [ ] Resource exists but wrong namespace specified

### data.k8sconnect_yaml_split
- [ ] Empty YAML input (content = "")
- [ ] Invalid YAML syntax
- [ ] YAML with no resources (only comments)
- [ ] YAML with unsupported resource types
- [ ] Empty pattern (no files match)
- [ ] Invalid kustomize path (directory doesn't exist)
- [ ] Kustomize path without kustomization.yaml
- [ ] Kustomize with broken configuration (invalid patch)
- [ ] Valid kustomize build (happy path)

### data.k8sconnect_yaml_scoped
- [ ] Empty YAML input (content = "")
- [ ] Invalid YAML syntax
- [ ] YAML with no cluster-scoped resources
- [ ] YAML with no namespaced resources
- [ ] Empty pattern (no files match)
- [ ] Invalid kustomize path (directory doesn't exist)
- [ ] Kustomize with all three categories (CRDs + cluster + namespaced)
- [ ] Valid kustomize build categorization (verify CRDs separate from CRs)

---

## Phase 5: Connection/Auth Error Testing

### Connection Issues
- [ ] Invalid cluster host
- [ ] Invalid port
- [ ] Cluster doesn't exist
- [ ] Network timeout

### Auth Issues
- [ ] Invalid client certificate
- [ ] Invalid client key
- [ ] Invalid CA certificate
- [ ] Invalid token
- [ ] Expired credentials

### ADR-023: Resilient Read Auth Behavior
- [ ] Test with `token` â€” invalid token during Read should produce WARNING (not error)
  - [ ] Warning says "Using Prior State â€” Authentication Failed"?
  - [ ] Warning includes the actual error detail?
  - [ ] Warning suggests checking cluster authentication?
  - [ ] Resource NOT removed from state (prior state preserved)?
- [ ] Test k8sconnect_patch Read with expired token â€” warning, not error?
- [ ] Test k8sconnect_wait Read with expired token â€” warning, not error?
- [ ] Verify: auth errors during Create/Update/Delete are still HARD ERRORS (not warnings)
- [ ] Verify: connection errors (bad host, network timeout) are still HARD ERRORS during Read

---

## Phase 6: Edge Cases and Boundaries

### Large Payloads
- [ ] ConfigMap with 1MB of data
- [ ] Secret with large binary data
- [ ] Resource with 100 labels
- [ ] Resource with very long annotation values

### Special Characters
- [ ] Unicode in all text fields
- [ ] Emoji in labels/annotations
- [ ] Special YAML characters (quotes, colons, etc.)
- [ ] Newlines and tabs in data

### Concurrency and Race Conditions
- [ ] Multiple resources depending on same resource
- [ ] for_each with resource replacement
- [ ] Rapid apply/destroy cycles

### State Edge Cases
- [ ] Resource deleted outside Terraform (drift detection)
- [ ] Resource modified outside Terraform (drift warning)
- [ ] Import then modify
- [ ] Resource with finalizer (deletion behavior)

---

## Phase 7: Warning Message Quality

### Drift Warnings
- [ ] Field modified by kubectl (shows warning with manager)
- [ ] Field modified by HPA (correctly ignores if not owned)
- [ ] Multiple fields drifted (clear list)

### Ownership Warnings
- [ ] Patch taking ownership (only shows on actual change)
- [ ] Field ownership transition (clear explanation)

---

## Phase 8: UX Polish Verification

### ğŸš¨ UX RED FLAGS - These should NEVER happen:
- [ ] âŒ Generic "operation failed" without context
- [ ] âŒ Stack traces or panic messages shown to user
- [ ] âŒ Internal error codes without explanation
- [ ] âŒ "Contact your administrator" (we ARE the administrators!)
- [ ] âŒ Silent failures (operation fails but no error shown)
- [ ] âŒ Errors that blame the user without helping
- [ ] âŒ Inconsistent terminology (mixing "resource"/"object"/"manifest")
- [ ] âŒ Missing resource identification (which resource failed?)
- [ ] âŒ Timeout without showing current state
- [ ] âŒ "Unexpected" errors without guidance
- [ ] âŒ Errors and warnings that are pointlessly verbose. We don't need a book.
- [ ] âŒ Errors and warnings that contain redundent information on different lines.


### For EVERY error message found:
- [ ] Is the error title clear and specific?
- [ ] Does it explain what went wrong?
- [ ] Does it explain why it might have happened?
- [ ] Does it tell the user how to fix it?
- [ ] Are the suggestions actionable?
- [ ] Is the formatting clean and readable?
- [ ] Does it use user-friendly language (not internal jargon)?
- [ ] Does it provide kubectl commands when helpful?
- [ ] Does it show current state when relevant?
- [ ] Is it as concise as possible without sacrficing utility?
- [ ] Does it follow existing patterns?

### World-Class Error Examples:
âœ… GOOD: "Resource Already Exists: The ConfigMap 'app-config' already exists in namespace 'production'. To manage this existing resource, use 'terraform import k8sconnect_object.config <import-id>'"

âŒ BAD: "apply failed: conflict"

---

## Phase 9: Regression Testing

After ANY fix:
- [ ] Run full happy path again (62 resources)
- [ ] Verify zero-diff still works
- [ ] Run ALL error tests again
- [ ] Check for any new issues introduced

---

## Phase 10: Resource Modification Edge Cases

**Goal**: Verify updates work correctly and maintain zero-diff stability

### Scaling Tests
- [ ] Scale Deployment replicas (2 â†’ 5 â†’ 1)
- [ ] Run `terraform apply` after each change
- [ ] Verify zero-diff after scaling completes
- [ ] Scale StatefulSet replicas (1 â†’ 3 â†’ 2)
- [ ] Scale ReplicaSet replicas
- [ ] Verify HPA respects minReplicas/maxReplicas changes

### Image and Container Changes
- [ ] Change Deployment image (nginx:latest â†’ nginx:1.25)
- [ ] Add new environment variable to container
- [ ] Remove environment variable
- [ ] Modify existing environment variable value
- [ ] Change container command/args
- [ ] Add new container to pod spec
- [ ] Remove container from pod spec

### Labels and Annotations
- [ ] Add new label to existing resource
- [ ] Modify existing label value
- [ ] Remove label
- [ ] Add annotation with long value (test large data)
- [ ] Add annotation with special characters (unicode, emoji)
- [ ] Verify selector-related labels trigger replacement when appropriate

### Resource Limits and Requests
- [ ] Modify memory request (64Mi â†’ 128Mi)
- [ ] Modify CPU limit (100m â†’ 200m)
- [ ] Test quantity normalization:
Â Â - Change 32Mi to 33554432 (bytes equivalent)
Â Â - Run `terraform apply` - should show zero diff
Â Â - Change back to 32Mi - should show zero diff
- [ ] Add limits where none existed
- [ ] Remove limits

### ConfigMap and Secret Updates
- [ ] Modify ConfigMap data key
- [ ] Add new data key to ConfigMap
- [ ] Remove data key from ConfigMap
- [ ] Update Secret data (base64 encoded)
- [ ] Verify pods referencing them handle updates

### YAML Formatting Variations
- [ ] Change YAML style (flow â†’ block, quotes â†’ no quotes)
- [ ] Add comments to YAML
- [ ] Change indentation (2 spaces â†’ 4 spaces)
- [ ] Use different string quoting styles
- [ ] Verify all format changes show zero-diff (format shouldn't matter)

### Volume and Storage Changes
- [ ] Modify PVC storage size (note: may require new PVC if not expandable)
- [ ] Change StorageClass (requires replacement)
- [ ] Modify volume mount paths

### Delete Protection and Data Persistence
**Goal**: Verify critical resources can't be accidentally destroyed

#### PVC/PV Delete Protection Testing
- [ ] Create PVC with `delete_protection = true`:
Â Â ```hcl
Â Â resource "k8sconnect_object" "protected_pvc" {
Â Â Â Â yaml_body = <<-YAML
Â Â Â Â Â Â apiVersion: v1
Â Â Â Â Â Â kind: PersistentVolumeClaim
Â Â Â Â Â Â metadata:
Â Â Â Â Â Â Â Â name: protected-data
Â Â Â Â Â Â Â Â namespace: default
Â Â Â Â Â Â spec:
Â Â Â Â Â Â Â Â accessModes: ["ReadWriteOnce"]
Â Â Â Â Â Â Â Â resources:
Â Â Â Â Â Â Â Â Â Â requests:
Â Â Â Â Â Â Â Â Â Â Â Â storage: 1Gi
Â Â Â Â YAML
Â Â Â Â cluster = local.cluster
Â Â Â Â delete_protection = true
Â Â }
Â Â ```
- [ ] Verify PVC is created and bound
- [ ] Attempt `terraform destroy` - should FAIL with clear error
- [ ] âœ… Error message explains delete_protection is enabled?
- [ ] âœ… Error message shows how to disable protection?
- [ ] Set `delete_protection = false` and run `terraform apply`
- [ ] Now run `terraform destroy` - should succeed

#### StorageClass Reclaim Policy Testing
- [ ] Create StorageClass with `reclaimPolicy: Retain`:
Â Â ```hcl
Â Â resource "k8sconnect_object" "retain_storage" {
Â Â Â Â yaml_body = <<-YAML
Â Â Â Â Â Â apiVersion: storage.k8s.io/v1
Â Â Â Â Â Â kind: StorageClass
Â Â Â Â Â Â metadata:
Â Â Â Â Â Â Â Â name: retain-test
Â Â Â Â Â Â provisioner: rancher.io/local-pathÂ Â # or your cluster's provisioner
Â Â Â Â Â Â reclaimPolicy: Retain
Â Â Â Â YAML
Â Â Â Â cluster = local.cluster
Â Â }
Â Â ```
- [ ] Create PVC using this StorageClass
- [ ] Write test data to a pod using this PVC
- [ ] Remove PVC from Terraform config (or destroy)
- [ ] Verify PV still exists: `kubectl get pv`
- [ ] âœ… PV should be in "Released" state, not deleted
- [ ] Clean up manually: `kubectl delete pv <pv-name>`

#### Finalizer Handling
- [ ] Create resource with finalizer:
Â Â ```hcl
Â Â resource "k8sconnect_object" "finalized" {
Â Â Â Â yaml_body = <<-YAML
Â Â Â Â Â Â apiVersion: v1
Â Â Â Â Â Â kind: ConfigMap
Â Â Â Â Â Â metadata:
Â Â Â Â Â Â Â Â name: finalized-resource
Â Â Â Â Â Â Â Â namespace: default
Â Â Â Â Â Â Â Â finalizers:
Â Â Â Â Â Â Â Â Â Â - example.com/test-finalizer
Â Â Â Â Â Â data:
Â Â Â Â Â Â Â Â test: value
Â Â Â Â YAML
Â Â Â Â cluster = local.cluster
Â Â }
Â Â ```
- [ ] Attempt `terraform destroy`
- [ ] âœ… Destroy should hang waiting for finalizer to be removed
- [ ] âœ… Check timeout message quality (should explain finalizer blocking deletion)
- [ ] Manually remove finalizer: `kubectl patch configmap finalized-resource -p '{"metadata":{"finalizers":null}}'`
- [ ] Verify destroy completes successfully

#### Force Destroy Testing
- [ ] Create resource with finalizer AND `force_destroy = true`
- [ ] Run `terraform destroy`
- [ ] âœ… Should force-delete despite finalizer
- [ ] Verify resource is removed from cluster

---

## Phase 11: Comprehensive Drift Testing

**Goal**: Verify drift detection and correction for fields we own

### Prerequisites
- [ ] Ensure `KUBECONFIG=./kind-validation-config` is set or prefix commands
- [ ] Identify resources managed by k8sconnect (check managedFields)

### Replica Drift Tests
- [ ] Scale Deployment via kubectl: `kubectl scale deployment web-deployment --replicas=5 -n kind-validation`
- [ ] Run `terraform plan` - should detect drift (if replicas not ignored)
- [ ] Run `terraform apply` - should correct drift back to configured value
- [ ] Verify warning message mentions "kubectl" as the manager who modified it

### Label and Annotation Drift
- [ ] Add label via kubectl: `kubectl label deployment web-deployment test=drift -n kind-validation`
- [ ] Run `terraform plan` - should show drift
- [ ] Run `terraform apply` - should remove unexpected label
- [ ] Modify existing label via kubectl
- [ ] Verify drift detected and corrected

### Image Drift Tests
- [ ] Change container image via kubectl: `kubectl set image deployment/web-deployment nginx=nginx:1.24 -n kind-validation`
- [ ] Run `terraform plan` - should detect image drift
- [ ] Run `terraform apply` - should correct back to configured image
- [ ] Check error message quality

### Data Drift (ConfigMap/Secret)
- [ ] Modify ConfigMap data via kubectl: `kubectl edit configmap app-config -n kind-validation`
- [ ] Run `terraform plan` - should detect drift
- [ ] Run `terraform apply` - should restore correct data
- [ ] Modify Secret data via kubectl
- [ ] Verify drift detection and correction

### HPA Managed Fields (Ignore Test)
- [ ] If HPA is managing replicas, verify we DON'T show drift on replica count
- [ ] HPA modifies deployment replicas - should NOT trigger warning
- [ ] Verify ignore_fields works correctly for HPA scenario
- [ ] Check managedFields shows HPA as owner of spec.replicas

### Resource Deletion Drift
- [ ] Delete resource via kubectl: `kubectl delete deployment web-deployment -n kind-validation`
- [ ] Run `terraform plan` - should show resource needs to be created
- [ ] Run `terraform apply` - should recreate resource
- [ ] Verify resource comes back with correct configuration

### Multi-Field Drift
- [ ] Modify multiple fields via kubectl (replicas + image + labels)
- [ ] Run `terraform plan` - should show all drifted fields clearly
- [ ] Verify error message lists all changes with responsible managers
- [ ] Run `terraform apply` - should correct all drifts

### Ownership Transfer Testing
- [ ] Create resource with kubectl first
- [ ] Import into Terraform (covered in Phase 13)
- [ ] Modify via Terraform - should take ownership cleanly
- [ ] Check managedFields shows k8sconnect as owner

### Ownership Annotation Drift
- [ ] Remove k8sconnect ownership annotation via kubectl:
Â Â ```bash
Â Â kubectl annotate configmap <name> -n kind-validation k8sconnect.terraform.io/terraform-id-
Â Â ```
- [ ] Run `terraform plan` - should detect the missing annotation
- [ ] Verify WARNING message: "Resource Annotations Missing - Will Restore"
- [ ] Run `terraform apply` - should restore the annotation
- [ ] Verify annotation is restored:
Â Â ```bash
Â Â kubectl get configmap <name> -n kind-validation -o yaml | grep "k8sconnect.terraform.io/terraform-id"
Â Â ```
- [ ] Check warning message quality and clarity

---

## Phase 12: for_each Replacement Race Condition Test

**Goal**: Verify Delete() exits gracefully when ownership changes during wait

### Setup
- [ ] Identify a resource using for_each in main.tf (e.g., split_resources)
- [ ] Note the current for_each key (e.g., `"configmap.cluster-config"`)
- [ ] Verify resource is currently applied and stable

### Execute Replacement Test
- [ ] Change the for_each key to a NEW key
- [ ] Example: `"configmap.cluster-config"` â†’ `"configmap.default.cluster-config"`
- [ ] IMPORTANT: Both keys must map to the SAME Kubernetes object
Â Â - Same metadata.name
Â Â - Same metadata.namespace
Â Â - Only the Terraform resource identifier changes
- [ ] Run `terraform plan` - should show delete + create
- [ ] Run `terraform apply` with timer running
- [ ] Apply should complete in seconds (NOT timeout at 5 minutes)
- [ ] Monitor for "ownership changed" detection in logs

### Verification
- [ ] Verify resource still exists: `kubectl get configmap cluster-config -n kind-validation`
- [ ] Check resource data is correct (not recreated, just adopted)
- [ ] Run `terraform apply` again - should show zero diff
- [ ] Check managedFields - should show k8sconnect as manager

### Expected Behavior
- [ ] Delete() should detect ownership change during wait loop
- [ ] Should log graceful exit (not timeout error)
- [ ] Create() should adopt existing resource with updated data
- [ ] Total operation completes in < 30 seconds (not 5 minutes)

**If timeout occurs**: The race condition fix is broken!

---

## Phase 13: Comprehensive Import Testing

**Goal**: Test complete import workflow including config generation

### Setup Import Test Environment
- [ ] Create import test namespace: `kubectl create namespace import-test`
- [ ] Verify namespace exists: `kubectl get namespace import-test`

### Import Test 1: Deployment with Generated Config
- [ ] Create deployment via kubectl:
Â Â ```bash
Â Â kubectl apply -f - <<EOF
Â Â apiVersion: apps/v1
Â Â kind: Deployment
Â Â metadata:
Â Â Â Â name: import-test-deployment
Â Â Â Â namespace: import-test
Â Â Â Â labels:
Â Â Â Â Â Â app: import-test
Â Â spec:
Â Â Â Â replicas: 3
Â Â Â Â selector:
Â Â Â Â Â Â matchLabels:
Â Â Â Â Â Â Â Â app: import-test
Â Â Â Â template:
Â Â Â Â Â Â metadata:
Â Â Â Â Â Â Â Â labels:
Â Â Â Â Â Â Â Â Â Â app: import-test
Â Â Â Â Â Â spec:
Â Â Â Â Â Â Â Â containers:
Â Â Â Â Â Â Â Â - name: nginx
Â Â Â Â Â Â Â Â Â Â image: nginx:latest
Â Â Â Â Â Â Â Â Â Â resources:
Â Â Â Â Â Â Â Â Â Â Â Â requests:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â memory: "64Mi"
Â Â Â Â Â Â Â Â Â Â Â Â Â Â cpu: "50m"
Â Â EOF
Â Â ```
- [ ] Create new file `import-test.tf` with import block only:
Â Â ```hcl
Â Â import {
Â Â Â Â to = k8sconnect_object.imported_deployment
Â Â Â Â id = "apps/v1//Deployment/import-test/import-test-deployment"
Â Â }
Â Â ```
- [ ] Run `terraform plan -generate-config-out=generated.tf`
- [ ] Verify generated.tf was created and contains deployment config
- [ ] Edit generated.tf to add cluster connection (copy from main.tf)
- [ ] Run `terraform plan` - should show import with no changes after
- [ ] Run `terraform apply` - import should succeed

### Import Verification
- [ ] Check managedFields: `kubectl get deployment import-test-deployment -n import-test -o yaml | grep -A 5 managedFields`
- [ ] Verify "k8sconnect" appears as manager
- [ ] Verify "operation: Apply" and "fieldsType: FieldsV1"
- [ ] Run `terraform apply` again - should show zero diff (critical!)

### Import Test 2: Service Import
- [ ] Create service via kubectl
- [ ] Import using same workflow
- [ ] Verify zero-diff after import

### Import Test 3: ConfigMap Import
- [ ] Create configmap with multiple data keys via kubectl
- [ ] Import and verify all data is captured
- [ ] Verify zero-diff

### Post-Import Modification Tests
- [ ] Modify imported deployment (change replicas)
- [ ] Run `terraform apply` - should update successfully
- [ ] Verify k8sconnect maintains ownership
- [ ] Scale via kubectl (create drift)
- [ ] Verify Terraform detects and corrects drift

### Import Test 4: CRD Import (if applicable)
- [ ] Import existing Cactus CR
- [ ] Verify CRD-based resources import correctly
- [ ] Test updates to imported CR

### Cleanup Import Tests
- [ ] Remove import test resources from Terraform configs
- [ ] Run `terraform apply` - should destroy import-test resources
- [ ] Delete namespace: `kubectl delete namespace import-test`
- [ ] Remove import-test.tf and generated.tf files

---

## Phase 13.5: Collision Detection & Ownership Recovery Testing

**Goal**: Verify collision detection prevents silent adoption and annotation loss recovery works

### CREATE Collision Detection
- [ ] Create ConfigMap with kubectl:
Â Â ```bash
Â Â kubectl create configmap collision-test -n kind-validation --from-literal=key=value
Â Â ```
- [ ] Add same ConfigMap to Terraform config (without import):
Â Â ```hcl
Â Â resource "k8sconnect_object" "collision_test" {
Â Â Â Â yaml_body = <<-YAML
Â Â Â Â Â Â apiVersion: v1
Â Â Â Â Â Â kind: ConfigMap
Â Â Â Â Â Â metadata:
Â Â Â Â Â Â Â Â name: collision-test
Â Â Â Â Â Â Â Â namespace: kind-validation
Â Â Â Â Â Â data:
Â Â Â Â Â Â Â Â key: value
Â Â Â Â YAML
Â Â Â Â cluster = local.cluster
Â Â }
Â Â ```
- [ ] Run `terraform apply` - should ERROR with "Resource Already Exists"
- [ ] Verify error message includes import instructions
- [ ] Verify it does NOT silently adopt the resource
- [ ] Remove the collision_test resource from config

### Annotation Loss Recovery
- [ ] Create new ConfigMap via Terraform:
Â Â ```hcl
Â Â resource "k8sconnect_object" "annotation_test" {
Â Â Â Â yaml_body = <<-YAML
Â Â Â Â Â Â apiVersion: v1
Â Â Â Â Â Â kind: ConfigMap
Â Â Â Â Â Â metadata:
Â Â Â Â Â Â Â Â name: annotation-test
Â Â Â Â Â Â Â Â namespace: kind-validation
Â Â Â Â Â Â data:
Â Â Â Â Â Â Â Â test: annotation-recovery
Â Â Â Â YAML
Â Â Â Â cluster = local.cluster
Â Â }
Â Â ```
- [ ] Apply to create the resource
- [ ] Verify ownership annotation exists:
Â Â ```bash
Â Â kubectl get configmap annotation-test -n kind-validation -o yaml | grep "k8sconnect.terraform.io/terraform-id"
Â Â ```
- [ ] Remove annotation manually:
Â Â ```bash
Â Â kubectl annotate configmap annotation-test -n kind-validation k8sconnect.terraform.io/terraform-id-
Â Â ```
- [ ] Run `terraform plan` - should show changes needed
- [ ] Verify WARNING message about "Resource Annotations Missing - Will Restore"
- [ ] Run `terraform apply` - should restore annotations
- [ ] Verify annotation is back:
Â Â ```bash
Â Â kubectl get configmap annotation-test -n kind-validation -o yaml | grep "k8sconnect.terraform.io/terraform-id"
Â Â ```
- [ ] Run `terraform apply` again - should show zero diff

### Multi-State Collision Testing
- [ ] Create resource with Terraform in current workspace (state A)
- [ ] Create new workspace: `terraform workspace new collision-test`
- [ ] Add same resource to config in new workspace
- [ ] Run `terraform apply` - should ERROR with ownership conflict
- [ ] Verify error explains the terraform-id mismatch
- [ ] Switch back to default workspace: `terraform workspace select default`
- [ ] Clean up test workspace: `terraform workspace delete collision-test`

### Post-Collision Import Test
- [ ] After collision detection, use import to adopt kubectl-created resource:
Â Â ```bash
Â Â terraform import k8sconnect_object.collision_test "//v1/ConfigMap/kind-validation/collision-test"
Â Â ```
- [ ] Run `terraform apply` - should succeed and add ownership annotations
- [ ] Verify k8sconnect now owns the resource
- [ ] Clean up imported resource

---

## Phase 14: k8sconnect_helm_release Testing

**Goal**: Test Helm release lifecycle, error handling, UX quality, and edge cases.

**Setup**: Add helm_release resources to `scenarios/kind-validation/`. Use `test/testdata/charts/simple-test` (good image) and `test/testdata/charts/bad-image-test` (guaranteed failure) for testing.

### Prerequisites
- [ ] Helm CLI installed on system
- [ ] Provider rebuilt and installed: `make install`
- [ ] kind-validation cluster running
- [ ] Verify test charts exist in `test/testdata/charts/`

### 14.1 Happy Path and Lifecycle

Add to kind-validation:
```hcl
resource "k8sconnect_helm_release" "basic" {
  cluster     = local.cluster
  name        = "qa-basic"
  namespace   = "qa-helm"
  chart       = "../../test/testdata/charts/simple-test"
  create_namespace = true
  wait        = true
}
```

- [ ] `terraform plan` shows create, plan output is readable
- [ ] `terraform apply` succeeds
- [ ] Verify release installed: `helm list -n qa-helm --kubeconfig=./kind-validation-config`
- [ ] Verify pods running: `kubectl get pods -n qa-helm`
- [ ] Check computed attributes populated: `revision`, `status`, `manifest`, `chart_name`, `chart_version`
- [ ] `terraform plan` again shows **zero diff** (no unnecessary revision bumps)
- [ ] Update values (add `replicaCount: 2` via `values`), apply
- [ ] Verify revision incremented to 2
- [ ] Verify 2 pods running
- [ ] `terraform destroy`, verify clean uninstall
- [ ] `helm list -n qa-helm` shows nothing

### 14.2 Value Handling

**Precedence test** (values < set < set_list < set_sensitive):
```hcl
resource "k8sconnect_helm_release" "values_test" {
  cluster   = local.cluster
  name      = "qa-values"
  namespace = "qa-helm"
  chart     = "../../test/testdata/charts/simple-test"
  create_namespace = true

  values = yamlencode({
    replicaCount = 1
    image = { tag = "from-values" }
  })

  set = [{ name = "replicaCount", value = "3" }]
}
```

- [ ] Apply. Verify replicaCount=3 (set wins over values). Check with `helm get values qa-values -n qa-helm`
- [ ] Add `set_sensitive` with same key. Verify it wins over `set`
- [ ] Zero-diff on re-apply

**set_list with JSON array**:
```hcl
set_list = [{ name = "tolerations", value = jsonencode(["key=node:NoSchedule"]) }]
```
- [ ] Apply succeeds, value parsed as array
- [ ] Try comma-separated fallback: `value = "a,b,c"`. Does it split correctly?

**Escaped dots in keys**:
```hcl
set = [{ name = "nodeSelector.kubernetes\\.io/hostname", value = "worker-1" }]
```
- [ ] Apply succeeds, key treated as `nodeSelector["kubernetes.io/hostname"]` not deeply nested

**Edge cases**:
- [ ] Empty `values = ""`. Does it error or treat as no values?
- [ ] Malformed YAML in `values`. Error message quality? Does it show the parse error clearly?
- [ ] Very long values YAML (10KB+). Works without issue?
- [ ] Values with special characters (quotes, newlines, unicode)
- [ ] YAML anchors and aliases in values. Handled or clear error?

### 14.3 Namespace Handling

- [ ] Deploy with `create_namespace = true` to non-existent namespace. Namespace created?
- [ ] Deploy with `create_namespace = false` (default) to non-existent namespace. Error message quality? Does it say "namespace not found" clearly?
- [ ] Deploy to `kube-system`. Works?
- [ ] Destroy release with `create_namespace = true`. Namespace NOT deleted (Helm behavior). Is this surprising? Document UX.

### 14.4 Wait and Timeout Behavior

**wait = true (default)**:
- [ ] Deploy simple-test with `wait = true`. Apply waits for pods ready, then succeeds.
- [ ] Deploy bad-image-test with `wait = true, timeout = "30s"`. Fails at ~30s.
  - [ ] Error message quality: Shows pod status? Events? Suggests kubectl debug commands?
  - [ ] Does the error mention the actual image pull failure?

**wait = false**:
- [ ] Deploy bad-image-test with `wait = false`. Apply succeeds immediately (doesn't wait).
- [ ] Status in state reflects actual release status?

**Atomic / RollbackOnFailure**:
- [ ] Deploy bad-image-test with `atomic = true`. Fails and rolls back automatically.
- [ ] After rollback, `helm list` shows no stuck "failed" release?
- [ ] Terraform state reflects the failure (no phantom state)?

### 14.5 Delete Behavior

- [ ] Normal destroy of healthy release. Clean uninstall.
- [ ] Manually delete release with `helm uninstall`. Then `terraform destroy`. Handles gracefully (no error about "release not found")?
- [ ] `force_destroy = true`. Verify hooks skipped during uninstall.
- [ ] `disable_hooks = true`. Verify hooks skipped during uninstall.
- [ ] Both `force_destroy` and `disable_hooks`. No double-skip issue?

### 14.6 Failed Release Recovery

**Critical scenario: prior failed release blocks create**:
- [ ] Manually create a failed release: `helm install qa-fail ../../test/testdata/charts/bad-image-test -n qa-helm --wait --timeout 10s` (will fail)
- [ ] `helm list -n qa-helm` shows "failed" release
- [ ] Now add matching Terraform config and `terraform apply`
- [ ] Provider should detect failed release, uninstall it, then install fresh
- [ ] Verify success. No "release already exists" error.

**Failed upgrade recovery**:
- [ ] Deploy good release via Terraform
- [ ] Change to bad-image-test chart, apply (will fail)
- [ ] Change back to simple-test, apply again
- [ ] Provider should recover (CleanupOnFail). Verify final release is healthy.

### 14.7 Drift Detection

- [ ] Deploy via Terraform. Manually `helm upgrade` with different values. `terraform plan` detects drift?
- [ ] What exactly does the drift warning say? Is it actionable?
- [ ] Deploy via Terraform. Manually `helm rollback`. `terraform plan` detects revision change?
- [ ] Deploy via Terraform. Manually `helm uninstall`. `terraform plan` shows recreate needed?
- [ ] Apply after external delete. Reinstalls successfully?

### 14.8 ADR-023 Auth Resilience

- [ ] Deploy release successfully
- [ ] Expire/invalidate the cluster token (or use a token that will expire)
- [ ] Run `terraform plan` with expired token
- [ ] Should NOT hard-fail. Should show warning and preserve prior state.
- [ ] Warning message is clear about auth failure?
- [ ] After fixing token, `terraform plan` works normally again

### 14.9 Import

- [ ] Manually install: `helm install qa-import ../../test/testdata/charts/simple-test -n qa-helm`
- [ ] Import: `terraform import k8sconnect_helm_release.test "<context>:qa-helm:qa-import"`
- [ ] `terraform plan` shows zero diff? Or expected diff?
- [ ] What does the import ID format look like? Is it documented?
- [ ] Try importing non-existent release. Error message quality?
- [ ] Try importing with wrong namespace. Error message quality?

### 14.10 Chart Sources

- [ ] Local chart (relative path): `chart = "../../test/testdata/charts/simple-test"`. Works.
- [ ] Non-existent local path. Error message says what?
- [ ] Directory without Chart.yaml. Error message?
- [ ] OCI chart (public.ecr.aws). Works?
- [ ] Invalid repository URL. Error message quality?
- [ ] Chart name not found in repo. Error message quality?

### 14.11 Skip CRDs and Dependency Update

- [ ] `skip_crds = true`. CRDs not installed?
- [ ] `dependency_update = true` with chart that has dependencies. Dependencies downloaded?
- [ ] Modify chart, re-apply with `dependency_update = true`. Dependencies re-downloaded?

### 14.12 Error Message Quality

For EVERY error encountered during this entire phase, evaluate:
- [ ] Error clearly states what failed?
- [ ] Error explains why (not just a stack trace or Helm internal error)?
- [ ] Error suggests how to fix it or what to check?
- [ ] Error includes relevant context (release name, namespace, chart)?

**Helm-specific UX red flags to watch for**:
- Generic "command failed" with no context
- Raw Go error strings leaked to user
- Helm internal errors not translated to user terms
- Missing release name or namespace in error
- "release already exists" with no recovery guidance
- Timeout errors that don't mention pod status or events

---

## Phase 14a: HashiCorp Helm Provider Issues (CRITICAL - Release Blocker)

**Goal**: Verify we FIXED all known HashiCorp helm provider issues documented in ADR-022

**Context**: These are REAL production issues users hit with hashicorp/helm. We must prove our implementation doesn't have these problems.

### State Management Issues (CRITICAL)

**Issue #1669: Resources Randomly Removed from State**
- [ ] Deploy helm release successfully
- [ ] Run terraform apply 100+ times in loop
- [ ] Verify resource NEVER disappears from state file
- [ ] Check state file after each apply for release presence
- [ ] Success criteria: 100% state persistence across all applies

**Issue #472: Failed Releases Update State**
- [ ] Deploy release that will fail (bad image, crash loop)
- [ ] Verify release creation FAILS as expected
- [ ] Check terraform state - should NOT contain failed release
- [ ] Run terraform apply again - should retry (not show "no changes")
- [ ] Fix the error and apply - should succeed
- [ ] Verify state only updated after successful deploy

### Drift Detection Issues

**Issue #1349: No Drift Detection After Manual Rollback**
- [ ] Deploy release at revision 1
- [ ] Upgrade to revision 2 via Terraform
- [ ] Manually rollback to revision 1: `helm rollback <release> 1`
- [ ] Run `terraform plan` - MUST show drift detected
- [ ] Run `terraform apply` - MUST re-upgrade to revision 2
- [ ] Verify Terraform detects and corrects manual rollbacks

**Issue #1307: OCI Chart Drift Not Detected**
- [ ] Deploy chart from OCI registry with specific digest
- [ ] Note the digest in state
- [ ] Manually upgrade with different digest (same version tag)
- [ ] Run `terraform plan` - MUST detect digest change
- [ ] Success criteria: Digest changes trigger drift detection

### Wait Logic Issues

**Issue #1364: Doesn't Wait for DaemonSets**
- [ ] Create chart with DaemonSet workload
- [ ] Deploy with `wait = true`
- [ ] Verify Terraform WAITS until DaemonSet is ready on all nodes
- [ ] Check kubectl during deploy - should see "waiting for DaemonSet"
- [ ] Success criteria: Apply doesn't complete until DaemonSet ready

**Issue #672: First Deploy Always Succeeds (Timeout Ignored)**
- [ ] Deploy chart with bad image and timeout = 30s
- [ ] Verify FIRST deployment respects timeout and fails
- [ ] Should NOT succeed after timeout
- [ ] Error should occur at ~30s, not succeed
- [ ] Success criteria: Timeout enforced on first deployment

**Issue #463: Timeout Parameter Ignored**
- [ ] Deploy chart with wait = true and timeout = 10s
- [ ] Use image that takes 20s to become ready
- [ ] Verify deployment FAILS at ~10s (not 30s default)
- [ ] Try with timeout = 60s - should succeed
- [ ] Success criteria: User-specified timeout always respected

### Security Issues (CRITICAL - Cannot ship if these fail)

**Issue #1287: Sensitive Values Leaked in Metadata**
- [ ] Deploy with `set_sensitive = [{ name = "password", value = "secret123" }]`
- [ ] Run `terraform plan` - check output
- [ ] Verify "secret123" does NOT appear in ANY plan output
- [ ] Check metadata field - should be marked sensitive or excluded
- [ ] Run terraform show - verify secret not visible
- [ ] Success criteria: ZERO sensitive value exposure in any output

**Issue #1221: Sensitive Attribute Not Respected**
- [ ] Deploy with sensitive = true on secret values
- [ ] Verify state file marks values as sensitive
- [ ] Verify `terraform output` respects sensitivity
- [ ] Modify sensitive value - plan should show (sensitive value changed)
- [ ] Success criteria: Sensitivity propagated through entire lifecycle

### Import Issues

**Issue #1613: Cannot Import Existing Releases**
- [ ] Manually create release: `helm install test-import <chart>`
- [ ] Write terraform config for same release
- [ ] Import: `terraform import k8sconnect_helm_release.test <context>:<namespace>:<release>`
- [ ] Run `terraform plan` - MUST show zero diff (no description field drift)
- [ ] Run `terraform apply` - MUST succeed without changes
- [ ] Success criteria: Clean import with no permanent drift

### OCI and Chart Issues

**Issue #1596: Digest-Based Charts Not Supported**
- [ ] Deploy chart with version = "1.0.0@sha256:abc123..."
- [ ] Verify deployment succeeds
- [ ] Deploy chart with ONLY digest (no version tag)
- [ ] Both must work without validation errors
- [ ] Success criteria: Full digest support for immutable deploys

**Issue #1645/#1660: OCI Registry Authentication**
- [ ] Test OCI chart from public registry (public.ecr.aws)
- [ ] Test OCI chart requiring auth (if available)
- [ ] Verify auth tokens refresh properly
- [ ] No intermittent auth failures
- [ ] Success criteria: Reliable OCI authentication

### Dependency Management

**Issue #576: Dependencies Not Downloaded on Local Chart Update**
- [ ] Create local chart with Chart.yaml dependencies
- [ ] Deploy with `dependency_update = true`
- [ ] Modify chart (change values, version)
- [ ] Run terraform apply
- [ ] Verify dependencies RE-DOWNLOADED (not stale)
- [ ] Success criteria: dependency_update always works on chart changes

### Values Handling

**Issue #524: Values and Set Arguments Mixed, Changes Ignored**
- [ ] Deploy with both `values` YAML and `set` parameters
- [ ] Modify only the `set` parameter
- [ ] Run terraform plan - MUST show the set change
- [ ] Run terraform apply - MUST apply set change
- [ ] Verify precedence: set/set_sensitive > values YAML
- [ ] Success criteria: All value sources work together correctly

**Issue #906: Manifest Always Triggers Recreate**
- [ ] Deploy helm release
- [ ] Run terraform plan (no config changes)
- [ ] MUST show "No changes" (not revision increment)
- [ ] Apply 10 times in a row
- [ ] Verify revision stays the same
- [ ] Success criteria: No unnecessary revision increments

### Acceptance Criteria

**ALL of the above tests must PASS before we can release helm_release.**

If ANY test fails:
1. Document the failure in QA-RESULTS.md
2. Fix the implementation
3. Re-run ALL tests
4. Do NOT proceed until 100% pass rate

---

## Phase 14b: Bootstrap and Unknown Value Handling (CRITICAL)

**Goal**: Verify helm_release handles bootstrap scenarios like k8sconnect_object

**Context**: This provider's PRIMARY differentiator is bootstrapping clusters + workloads in one apply. Unknown value handling MUST work perfectly.

### Unknown Cluster Connection Values

**Scenario 1: Cluster doesn't exist yet**
- [ ] Create TF config with EKS/kind cluster + helm release in same config
- [ ] Cluster endpoint is "known after apply"
- [ ] Cluster CA cert is "known after apply"
- [ ] Run `terraform plan` - should NOT error
- [ ] Plan should show helm release will be created (not fail)
- [ ] Should defer evaluation until apply
- [ ] Run `terraform apply` - cluster created first, then helm release
- [ ] Verify helm release deploys successfully after cluster exists

**Scenario 2: Cluster exists, connection values known**
- [ ] Use existing cluster with known endpoint/certs
- [ ] Deploy helm release with all values known at plan time
- [ ] Should do full validation during plan
- [ ] Should show accurate preview of what will be deployed
- [ ] No "known after apply" for manifests or outputs

### Unknown Values in Chart/Repository

**Scenario 3: Chart version from data source**
- [ ] Chart version comes from data source: `data.helm_repo.app.version`
- [ ] Version is unknown at plan time
- [ ] Plan should succeed (defer to apply)
- [ ] Apply should resolve version and deploy
- [ ] No errors about unknown values

**Scenario 4: Repository URL from resource**
- [ ] Repository URL comes from another resource (e.g., ECR registry URL)
- [ ] URL is "known after apply"
- [ ] Plan must succeed
- [ ] Apply must resolve and deploy
- [ ] Verify chart downloaded from computed URL

### Unknown Values in Helm Values

**Scenario 5: Values contain unknown interpolations**
- [ ] values YAML contains ${resource.output}
- [ ] Output is "known after apply"
- [ ] Plan should succeed (not error on YAML parsing)
- [ ] Apply should substitute values and deploy
- [ ] Verify deployed resources have correct substituted values

**Scenario 6: Set parameters with unknown values**
- [ ] set = [{ name = "foo", value = aws_resource.id }]
- [ ] Value is "known after apply"
- [ ] Plan should show (known after apply) for affected resources
- [ ] Apply should resolve and deploy correctly
- [ ] Success criteria: Defers gracefully, no errors

**Scenario 7: Sensitive values with unknowns**
- [ ] set_sensitive with values from secrets manager (unknown at plan)
- [ ] Should defer and mark as sensitive
- [ ] Apply should resolve without exposing values
- [ ] Verify no leakage in logs

### Comparison with k8sconnect_object Behavior

**Consistency Test: Same Unknown Handling**
- [ ] Create k8sconnect_object with unknown cluster
- [ ] Create k8sconnect_helm_release with unknown cluster
- [ ] BOTH should handle unknowns identically
- [ ] Same "known after apply" behavior
- [ ] Same deferral semantics
- [ ] No surprising differences between resources

### Bootstrap Workflow Integration Test

**End-to-End Bootstrap Test**
- [ ] Create complete bootstrap scenario:
Â Â - [ ] kind_cluster resource (cluster created)
Â Â - [ ] k8sconnect_helm_release (installs CNI - Cilium)
Â Â - [ ] k8sconnect_helm_release (installs cert-manager)
Â Â - [ ] k8sconnect_object (creates CRD-based resources using cert-manager CRDs)
- [ ] ALL in same terraform apply (no two-phase)
- [ ] Run `terraform plan` - no errors
- [ ] Run `terraform apply` - everything deploys in correct order
- [ ] Verify:
Â Â - [ ] Cluster created first
Â Â - [ ] CNI deployed second
Â Â - [ ] Cert-manager deployed third
Â Â - [ ] CRD resources created last
- [ ] Success criteria: Single apply, correct dependency order, no errors

### Unknown Value Error Handling

**Error Scenario: Chart doesn't exist (unknown at plan)**
- [ ] Chart path comes from unknown value
- [ ] At apply time, path resolves to non-existent chart
- [ ] Should show clear error during apply (not cryptic unknown value error)
- [ ] Error should explain the resolved value failed

**Error Scenario: Invalid values after substitution**
- [ ] Values YAML has unknown interpolation
- [ ] At apply time, substitution creates invalid YAML
- [ ] Should show clear YAML parsing error
- [ ] Should show the resolved YAML causing the error

### Acceptance Criteria for Bootstrap Support

**MUST PASS:**
1. [ ] Unknown cluster connection values don't cause plan errors
2. [ ] Unknown chart/repo/version values defer gracefully
3. [ ] Unknown values in helm values YAML don't break parsing
4. [ ] Complete bootstrap (cluster + helm) works in single apply
5. [ ] Behavior matches k8sconnect_object unknown handling
6. [ ] Error messages remain clear even with unknown values

**If ANY fail**: This breaks the core value proposition. FIX BEFORE RELEASE.

---

## Phase 15: Cleanup and Documentation

### Final Cleanup
- [ ] Run `terraform destroy` from kind-validation directory
- [ ] Watch output - all resources destroyed cleanly?
- [ ] Check for any stuck resources with finalizers
- [ ] Verify cluster is clean: `KUBECONFIG=./kind-validation-config kubectl get all -A`
- [ ] Check for any remaining namespaces: `kubectl get namespaces`
- [ ] Verify only kube-system, default, local-path-storage remain

### Comprehensive Documentation
- [ ] Open/create `QA-RESULTS.md` in project root
- [ ] Document test round metadata:
Â Â - Date and time
Â Â - Provider version tested
Â Â - Terraform version
Â Â - Kubernetes version (kind node)

### ğŸ“ UX Issues (DOCUMENT THESE FIRST!)
- [ ] **Poor Error Messages** - Include exact text and suggested improvement
Â Â - What it said vs. what it SHOULD say
Â Â - Missing context that would help
Â Â - Jargon that could be simplified
- [ ] **Surprising Behavior** - Anything that wasn't expected
Â Â - Silent adoptions/failures
Â Â - Unexpected state changes
Â Â - Confusing operation order
- [ ] **Missing Guidance** - Where users get stuck
Â Â - Errors without solutions
Â Â - Warnings without context
Â Â - Timeouts without current state

### Other Issues to Document:
- [ ] Bugs (unexpected behavior, crashes, panics)
- [ ] Performance issues (slow operations, timeouts)
- [ ] Document error message quality issues:
Â Â - Missing context (what/why/how to fix)
Â Â - Unclear language or jargon
Â Â - Poor formatting
Â Â - Missing kubectl command suggestions
- [ ] Document what WORKED WELL:
Â Â - Features that exceeded expectations
Â Â - Great error messages
Â Â - Smooth workflows
Â Â - Helpful warnings

### Quality Questions to Answer
- [ ] Were there any surprising behaviors?
- [ ] Are there UX improvements that would help users?
- [ ] Did you encounter any workarounds needed?
- [ ] Are there missing features that would be valuable?
- [ ] How was the overall developer experience?

### Final Verification
- [ ] Review QA-RESULTS.md for completeness
- [ ] Ensure every issue has:
Â Â - Clear description
Â Â - Steps to reproduce
Â Â - Expected vs actual behavior
Â Â - Severity (blocker/major/minor/enhancement)

---

## Completion Criteria

âœ… Checklist is complete when:
1. EVERY checkbox above is checked (Phases 0-14)
2. EVERY error message has been evaluated for quality
3. EVERY issue found has been documented in QA-RESULTS.md
4. ZERO items remain untested
5. Documentation is thorough and actionable

### ğŸ† UX Quality Bar
Before signing off, ask yourself:
- Would a Kubernetes beginner understand every error?
- Are there ANY surprises in the behavior?
- Does every error help the user succeed?
- Would you be proud to show this UX to customers?
- Is this genuinely BETTER than competing providers?

**If any answer is "no", we're not done.**

DO NOT declare "done" until this is 100% complete.

---

## Quick Reference Commands

```bash
# Build and install provider
cd /Users/jonathan/code/terraform-provider-k8sconnect
make install

# Setup test environment
cd scenarios/kind-validation
./reset.sh

# Terraform workflow
terraform init
terraform plan
terraform apply
terraform destroy

# Kubectl with kind cluster
export KUBECONFIG=./kind-validation-config
kubectl get all -A
kubectl get pods -n kind-validation
kubectl describe deployment <name> -n kind-validation
kubectl logs <pod-name> -n kind-validation

# Create drift
kubectl scale deployment web-deployment --replicas=5 -n kind-validation
kubectl label deployment web-deployment test=drift -n kind-validation
kubectl set image deployment/web-deployment nginx=nginx:1.24 -n kind-validation

# Check managedFields
kubectl get deployment <name> -n kind-validation -o yaml | grep -A 20 managedFields
```
