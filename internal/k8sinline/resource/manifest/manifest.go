// internal/k8sinline/resource/manifest/manifest.go
package manifest

import (
	"context"
	"crypto/sha256"
	"encoding/base64"
	"encoding/hex"
	"fmt"
	"os"
	"path/filepath"
	"reflect"
	"strings"
	"time"

	"github.com/hashicorp/terraform-plugin-framework/attr"
	"github.com/hashicorp/terraform-plugin-framework/resource"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/planmodifier"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/stringplanmodifier"
	"github.com/hashicorp/terraform-plugin-framework/types"
	"github.com/hashicorp/terraform-plugin-framework/types/basetypes"
	"github.com/hashicorp/terraform-plugin-log/tflog"
	"k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	k8sschema "k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
	clientcmdapi "k8s.io/client-go/tools/clientcmd/api"
	"sigs.k8s.io/yaml"

	"github.com/jmorris0x0/terraform-provider-k8sinline/internal/k8sinline/k8sclient"
)

var _ resource.Resource = (*manifestResource)(nil)

var _ resource.ResourceWithConfigValidators = (*manifestResource)(nil)

// ClientGetter function type for dependency injection
type ClientGetter func(ClusterConnectionModel) (k8sclient.K8sClient, error)

// ClusterConnectionModel is exported for use in provider
type ClusterConnectionModel struct {
	Host                 types.String   `tfsdk:"host"`
	ClusterCACertificate types.String   `tfsdk:"cluster_ca_certificate"`
	KubeconfigFile       types.String   `tfsdk:"kubeconfig_file"`
	KubeconfigRaw        types.String   `tfsdk:"kubeconfig_raw"`
	Context              types.String   `tfsdk:"context"`
	Exec                 *execAuthModel `tfsdk:"exec"`
}

type execAuthModel struct {
	APIVersion types.String   `tfsdk:"api_version"`
	Command    types.String   `tfsdk:"command"`
	Args       []types.String `tfsdk:"args"`
}

type manifestResource struct {
	clientGetter ClientGetter
}

type manifestResourceModel struct {
	ID                types.String `tfsdk:"id"`
	YAMLBody          types.String `tfsdk:"yaml_body"`
	ClusterConnection types.Object `tfsdk:"cluster_connection"`
	DeleteProtection  types.Bool   `tfsdk:"delete_protection"`
	DeleteTimeout     types.String `tfsdk:"delete_timeout"`
	ForceDestroy      types.Bool   `tfsdk:"force_destroy"`
}

// NewManifestResource creates a new manifest resource (backward compatibility)
func NewManifestResource() resource.Resource {
	return &manifestResource{
		clientGetter: CreateK8sClientFromConnection,
	}
}

// NewManifestResourceWithClientGetter creates a manifest resource with custom client getter
func NewManifestResourceWithClientGetter(getter ClientGetter) resource.Resource {
	return &manifestResource{
		clientGetter: getter,
	}
}

// CreateK8sClientFromConnection creates a K8sClient from connection model (exported for provider use)
func CreateK8sClientFromConnection(conn ClusterConnectionModel) (k8sclient.K8sClient, error) {
	r := &manifestResource{}
	return r.createK8sClient(conn)
}

func (r *manifestResource) Metadata(ctx context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) {
	resp.TypeName = req.ProviderTypeName + "_manifest"
}

// Enhanced Schema method with single nested attribute instead of block
func (r *manifestResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) {
	resp.Schema = schema.Schema{
		Description: "Applies a single‑document Kubernetes YAML manifest to a cluster, with per‑resource inline or kubeconfig‑based connection settings.",
		Attributes: map[string]schema.Attribute{
			"id": schema.StringAttribute{
				Computed: true,
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.UseStateForUnknown(),
				},
				Description: "Unique identifier for this manifest (generated by the provider).",
			},
			"yaml_body": schema.StringAttribute{
				Required:    true,
				Description: "UTF‑8 encoded, single‑document Kubernetes YAML. Multi‑doc files will fail validation.",
			},
			"delete_protection": schema.BoolAttribute{
				Optional:    true,
				Description: "When enabled, prevents Terraform from deleting this resource. Must be disabled before destruction. Defaults to false.",
			},
			"delete_timeout": schema.StringAttribute{
				Optional:    true,
				Description: "Maximum time to wait for resource deletion. Defaults to '5m' for most resources, '10m' for Namespaces/PVs. Examples: '1m', '10m', '1h'. Set '0' to skip waiting (not recommended).",
			},
			"force_destroy": schema.BoolAttribute{
				Optional:    true,
				Description: "When enabled, removes finalizers to force deletion if normal deletion times out. ⚠️ WARNING: May cause data loss. Use only when you understand the implications. Defaults to false.",
			},
			"cluster_connection": schema.SingleNestedAttribute{
				Required:    true,
				Description: "Connection settings for the target cluster. Exactly one of inline, kubeconfig_file or kubeconfig_raw must be populated.",
				Attributes: map[string]schema.Attribute{
					"host": schema.StringAttribute{
						Optional:    true,
						Sensitive:   true,
						Description: "Kubernetes API server endpoint (e.g. https://example.com). Required for inline mode.",
					},
					"cluster_ca_certificate": schema.StringAttribute{
						Optional:    true,
						Sensitive:   true,
						Description: "PEM‑encoded CA certificate bundle for the API server. Required for inline mode.",
					},
					"kubeconfig_file": schema.StringAttribute{
						Optional:    true,
						Sensitive:   true,
						Description: "Filesystem path to an existing kubeconfig file. Always supports live diffing.",
					},
					"kubeconfig_raw": schema.StringAttribute{
						Optional:    true,
						Sensitive:   true,
						Description: "Raw kubeconfig YAML content (CI‑friendly).",
					},
					"context": schema.StringAttribute{
						Optional:    true,
						Sensitive:   true,
						Description: "Context name within the provided kubeconfig (file or raw).",
					},
					"exec": schema.SingleNestedAttribute{
						Description: "Inline exec‑auth configuration for dynamic credentials. Must include api_version and command; args is optional.",
						Optional:    true,
						Sensitive:   true,
						Attributes: map[string]schema.Attribute{
							"api_version": schema.StringAttribute{
								Optional:    true,
								Description: "Authentication API version (e.g., 'client.authentication.k8s.io/v1').",
							},
							"command": schema.StringAttribute{
								Optional:    true,
								Description: "Executable command (e.g., 'aws', 'gcloud').",
							},
							"args": schema.ListAttribute{
								ElementType: types.StringType,
								Optional:    true,
								Description: "Command arguments (e.g., ['eks', 'get-token', '--cluster-name', 'my-cluster']).",
							},
						},
					},
				},
			},
		},
	}
}

func (r *manifestResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) {
	var data manifestResourceModel

	diags := req.Config.Get(ctx, &data)
	resp.Diagnostics.Append(diags...)
	if resp.Diagnostics.HasError() {
		return
	}

	// Check if cluster connection is ready (handles unknown values during planning)
	if !r.isConnectionReady(data.ClusterConnection) {
		resp.Diagnostics.AddError(
			"Cluster Connection Not Ready",
			"Cluster connection contains unknown values. This usually happens during planning when dependencies are not yet resolved.",
		)
		return
	}

	// Convert to connection model
	conn, err := r.convertObjectToConnectionModel(ctx, data.ClusterConnection)
	if err != nil {
		resp.Diagnostics.AddError("Connection Conversion Failed", err.Error())
		return
	}

	// Parse YAML into unstructured object
	obj, err := r.parseYAML(data.YAMLBody.ValueString())
	if err != nil {
		resp.Diagnostics.AddError("Invalid YAML", fmt.Sprintf("Failed to parse YAML: %s", err))
		return
	}

	// Create K8s client from cluster connection (now with caching)
	client, err := r.clientGetter(conn)
	if err != nil {
		resp.Diagnostics.AddError("Connection Failed", fmt.Sprintf("Failed to create Kubernetes client: %s", err))
		return
	}

	// Generate resource ID (moved up to use in annotation)
	id := r.generateID(obj, conn)
	data.ID = types.StringValue(id)

	// Set ownership annotation before applying
	annotations := obj.GetAnnotations()
	if annotations == nil {
		annotations = make(map[string]string)
	}
	annotations["k8sinline.terraform.io/id"] = data.ID.ValueString()
	obj.SetAnnotations(annotations)

	// Apply the manifest using server-side apply
	err = client.SetFieldManager("k8sinline").Apply(ctx, obj, k8sclient.ApplyOptions{
		FieldManager: "k8sinline",
		Force:        false,
	})
	if err != nil {
		resourceDesc := fmt.Sprintf("%s %s", obj.GetKind(), obj.GetName())
		severity, title, detail := r.classifyK8sError(err, "Create", resourceDesc)
		if severity == "warning" {
			resp.Diagnostics.AddWarning(title, detail)
		} else {
			resp.Diagnostics.AddError(title, detail)
		}
		return
	}

	tflog.Trace(ctx, "applied manifest", map[string]interface{}{
		"id":        data.ID.ValueString(),
		"kind":      obj.GetKind(),
		"name":      obj.GetName(),
		"namespace": obj.GetNamespace(),
	})

	diags = resp.State.Set(ctx, &data)
	resp.Diagnostics.Append(diags...)
}

func (r *manifestResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) {
	var data manifestResourceModel

	diags := req.State.Get(ctx, &data)
	resp.Diagnostics.Append(diags...)
	if resp.Diagnostics.HasError() {
		return
	}

	// Check if cluster connection is ready
	if !r.isConnectionReady(data.ClusterConnection) {
		tflog.Info(ctx, "Skipping Read due to unknown connection values", map[string]interface{}{
			"resource_id":        data.ID.ValueString(),
			"connection_unknown": true,
		})

		return
	}

	// Convert to connection model
	conn, err := r.convertObjectToConnectionModel(ctx, data.ClusterConnection)
	if err != nil {
		resp.Diagnostics.AddError("Connection Conversion Failed", err.Error())
		return
	}

	// Parse YAML to get object metadata
	obj, err := r.parseYAML(data.YAMLBody.ValueString())
	if err != nil {
		resp.Diagnostics.AddError("Invalid YAML", fmt.Sprintf("Failed to parse YAML: %s", err))
		return
	}

	// Create K8s client from cluster connection (cached)
	client, err := r.clientGetter(conn)
	if err != nil {
		resp.Diagnostics.AddError("Connection Failed", fmt.Sprintf("Failed to create Kubernetes client: %s", err))
		return
	}

	// Get GVR for the object
	gvr, err := r.getGVR(ctx, client, obj)
	if err != nil {
		resp.Diagnostics.AddError("Resource Discovery Failed", fmt.Sprintf("Failed to determine resource type: %s", err))
		return
	}

	// Check if object still exists
	_, err = client.Get(ctx, gvr, obj.GetNamespace(), obj.GetName())
	if err != nil {
		if errors.IsNotFound(err) {
			// Object no longer exists - remove from state
			resp.State.RemoveResource(ctx)
			return
		}
		// Other errors should be reported
		resourceDesc := fmt.Sprintf("%s %s", obj.GetKind(), obj.GetName())
		severity, title, detail := r.classifyK8sError(err, "Read", resourceDesc)
		if severity == "warning" {
			resp.Diagnostics.AddWarning(title, detail)
		} else {
			resp.Diagnostics.AddError(title, detail)
		}
		return
	}

	// Object exists - keep current state
	diags = resp.State.Set(ctx, &data)
	resp.Diagnostics.Append(diags...)
}

func (r *manifestResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) {
	var data manifestResourceModel
	diags := req.Plan.Get(ctx, &data)
	resp.Diagnostics.Append(diags...)
	if resp.Diagnostics.HasError() {
		return
	}

	// Check if cluster connection is ready
	if !r.isConnectionReady(data.ClusterConnection) {
		resp.Diagnostics.AddError(
			"Cluster Connection Not Ready",
			"Cluster connection contains unknown values. This usually happens during planning when dependencies are not yet resolved.",
		)
		return
	}

	// Convert plan connection to model
	conn, err := r.convertObjectToConnectionModel(ctx, data.ClusterConnection)
	if err != nil {
		resp.Diagnostics.AddError("Connection Conversion Failed", err.Error())
		return
	}

	// Get prior state to check for connection changes
	var priorState manifestResourceModel
	diags = req.State.Get(ctx, &priorState)
	resp.Diagnostics.Append(diags...)
	if resp.Diagnostics.HasError() {
		return
	}

	// Check connection changes only if both states are ready
	if r.isConnectionReady(priorState.ClusterConnection) {
		priorConn, err := r.convertObjectToConnectionModel(ctx, priorState.ClusterConnection)
		if err == nil {
			if r.anyConnectionFieldChanged(conn, priorConn) {
				if err := r.validateOwnership(ctx, data); err != nil {
					resp.Diagnostics.AddError("Connection Change Blocked", err.Error())
					return
				}
				resp.Diagnostics.AddWarning("Connection Changed",
					"Connection details changed. Verified target cluster via ownership annotation.")
			}
		}
	}

	// Parse YAML into unstructured object
	obj, err := r.parseYAML(data.YAMLBody.ValueString())
	if err != nil {
		resp.Diagnostics.AddError("Invalid YAML", fmt.Sprintf("Failed to parse YAML: %s", err))
		return
	}

	// Create K8s client from cluster connection (cached)
	client, err := r.clientGetter(conn)
	if err != nil {
		resp.Diagnostics.AddError("Connection Failed", fmt.Sprintf("Failed to create Kubernetes client: %s", err))
		return
	}

	// Set ownership annotation before applying
	annotations := obj.GetAnnotations()
	if annotations == nil {
		annotations = make(map[string]string)
	}
	annotations["k8sinline.terraform.io/id"] = data.ID.ValueString()
	obj.SetAnnotations(annotations)

	// Apply the updated manifest (server-side apply is idempotent)
	err = client.SetFieldManager("k8sinline").Apply(ctx, obj, k8sclient.ApplyOptions{
		FieldManager: "k8sinline",
		Force:        false,
	})
	if err != nil {
		resourceDesc := fmt.Sprintf("%s %s", obj.GetKind(), obj.GetName())
		severity, title, detail := r.classifyK8sError(err, "Update", resourceDesc)
		if severity == "warning" {
			resp.Diagnostics.AddWarning(title, detail)
		} else {
			resp.Diagnostics.AddError(title, detail)
		}
		return
	}

	tflog.Trace(ctx, "updated manifest", map[string]interface{}{
		"id":        data.ID.ValueString(),
		"kind":      obj.GetKind(),
		"name":      obj.GetName(),
		"namespace": obj.GetNamespace(),
	})

	diags = resp.State.Set(ctx, &data)
	resp.Diagnostics.Append(diags...)
}

func (r *manifestResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) {
	var data manifestResourceModel

	diags := req.State.Get(ctx, &data)
	resp.Diagnostics.Append(diags...)
	if resp.Diagnostics.HasError() {
		return
	}

	// Check delete protection first (doesn't require connection)
	if !data.DeleteProtection.IsNull() && data.DeleteProtection.ValueBool() {
		resp.Diagnostics.AddError(
			"Resource Protected from Deletion",
			"This resource has delete_protection enabled. To delete this resource, first set delete_protection = false in your configuration, run terraform apply, then run terraform destroy.",
		)
		return
	}

	// Check if cluster connection is ready
	if !r.isConnectionReady(data.ClusterConnection) {
		resp.Diagnostics.AddError(
			"Cluster Connection Not Ready",
			"Cannot delete resource: cluster connection contains unknown values.",
		)
		return
	}

	// Convert to connection model
	conn, err := r.convertObjectToConnectionModel(ctx, data.ClusterConnection)
	if err != nil {
		resp.Diagnostics.AddError("Connection Conversion Failed", err.Error())
		return
	}

	// Parse YAML to get object metadata
	obj, err := r.parseYAML(data.YAMLBody.ValueString())
	if err != nil {
		resp.Diagnostics.AddError("Invalid YAML", fmt.Sprintf("Failed to parse YAML: %s", err))
		return
	}

	// Create K8s client from cluster connection (cached)
	client, err := r.clientGetter(conn)
	if err != nil {
		resp.Diagnostics.AddError("Connection Failed", fmt.Sprintf("Failed to create Kubernetes client: %s", err))
		return
	}

	// Get GVR for the object
	gvr, err := r.getGVR(ctx, client, obj)
	if err != nil {
		resp.Diagnostics.AddError("Resource Discovery Failed", fmt.Sprintf("Failed to determine resource type: %s", err))
		return
	}

	// Check if resource exists before attempting deletion
	_, err = client.Get(ctx, gvr, obj.GetNamespace(), obj.GetName())
	if err != nil {
		if errors.IsNotFound(err) {
			// Object already gone - that's fine
			tflog.Trace(ctx, "object already deleted", map[string]interface{}{
				"id":        data.ID.ValueString(),
				"kind":      obj.GetKind(),
				"name":      obj.GetName(),
				"namespace": obj.GetNamespace(),
			})
			return
		}
		// Other errors should be reported
		resourceDesc := fmt.Sprintf("%s %s", obj.GetKind(), obj.GetName())
		severity, title, detail := r.classifyK8sError(err, "Delete", resourceDesc)
		if severity == "warning" {
			resp.Diagnostics.AddWarning(title, detail)
		} else {
			resp.Diagnostics.AddError(title, detail)
		}
		return
	}

	// Initiate deletion
	err = client.Delete(ctx, gvr, obj.GetNamespace(), obj.GetName(), k8sclient.DeleteOptions{})
	if err != nil && !errors.IsNotFound(err) {
		resourceDesc := fmt.Sprintf("%s %s", obj.GetKind(), obj.GetName())
		severity, title, detail := r.classifyK8sError(err, "Delete", resourceDesc)
		if severity == "warning" {
			resp.Diagnostics.AddWarning(title, detail)
		} else {
			resp.Diagnostics.AddError(title, detail)
		}
		return
	}

	// Wait for normal deletion to complete
	timeout := r.getDeleteTimeout(data)
	forceDestroy := !data.ForceDestroy.IsNull() && data.ForceDestroy.ValueBool()

	tflog.Debug(ctx, "Starting deletion wait", map[string]interface{}{
		"timeout":       timeout.String(),
		"force_destroy": forceDestroy,
	})

	err = r.waitForDeletion(ctx, client, gvr, obj, timeout)
	if err == nil {
		// Successful normal deletion
		tflog.Trace(ctx, "deleted manifest normally", map[string]interface{}{
			"id":        data.ID.ValueString(),
			"kind":      obj.GetKind(),
			"name":      obj.GetName(),
			"namespace": obj.GetNamespace(),
		})
		return
	}

	// Normal deletion failed/timed out - check if we should force destroy
	if !forceDestroy {
		// Not forcing, so report the timeout error with helpful guidance
		r.handleDeletionTimeout(resp, client, gvr, obj, timeout, err)
		return
	}

	// Force destroy enabled - remove finalizers and delete
	tflog.Warn(ctx, "Normal deletion failed, attempting force destroy", map[string]interface{}{
		"resource": fmt.Sprintf("%s/%s", obj.GetKind(), obj.GetName()),
		"timeout":  timeout.String(),
	})

	if err := r.forceDestroy(ctx, client, gvr, obj, resp); err != nil {
		resp.Diagnostics.AddError(
			"Force Destroy Failed",
			fmt.Sprintf("Failed to force destroy %s %s: %s", obj.GetKind(), obj.GetName(), err.Error()),
		)
		return
	}

	// Log successful force destroy
	tflog.Info(ctx, "Force destroyed manifest", map[string]interface{}{
		"id":        data.ID.ValueString(),
		"kind":      obj.GetKind(),
		"name":      obj.GetName(),
		"namespace": obj.GetNamespace(),
	})
}

// ImportState method implementing kubeconfig strategy
func (r *manifestResource) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) {
	// Parse import ID: "context/namespace/kind/name" or "context/kind/name" for cluster-scoped
	kubeContext, namespace, kind, name, err := r.parseImportID(req.ID)
	if err != nil {
		resp.Diagnostics.AddError(
			"Invalid Import ID",
			fmt.Sprintf("Expected format: <context>/<namespace>/<kind>/<name> or <context>/<kind>/<name>\n\nExamples:\n"+
				"  prod/default/Pod/nginx\n"+
				"  staging/kube-system/Service/coredns\n"+
				"  prod/Namespace/my-namespace\n"+
				"  dev/ClusterRole/admin\n\nError: %s", err.Error()),
		)
		return
	}

	// Validate required parts
	if kubeContext == "" {
		resp.Diagnostics.AddError(
			"Import Failed: Missing Context",
			"The import ID must include a kubeconfig context as the first part.\n\n"+
				"Format: <context>/<namespace>/<kind>/<name> or <context>/<kind>/<name>\n\n"+
				"Available contexts can be found with: kubectl config get-contexts",
		)
		return
	}
	if kind == "" {
		resp.Diagnostics.AddError(
			"Import Failed: Missing Kind",
			"The resource kind cannot be empty in the import ID.",
		)
		return
	}
	if name == "" {
		resp.Diagnostics.AddError(
			"Import Failed: Missing Name",
			"The resource name cannot be empty in the import ID.",
		)
		return
	}

	// Read kubeconfig from KUBECONFIG env var or default location
	kubeconfigPath := os.Getenv("KUBECONFIG")
	if kubeconfigPath == "" {
		homeDir := os.Getenv("HOME")
		if homeDir == "" {
			resp.Diagnostics.AddError(
				"Import Failed: KUBECONFIG Not Found",
				"KUBECONFIG environment variable is not set and HOME directory could not be determined.\n\n"+
					"Set KUBECONFIG environment variable:\n"+
					"  export KUBECONFIG=~/.kube/config\n"+
					"  terraform import k8sinline_manifest.example \"prod/default/Pod/nginx\"",
			)
			return
		}
		kubeconfigPath = filepath.Join(homeDir, ".kube", "config")
	}

	// Check if kubeconfig file exists
	if _, err := os.Stat(kubeconfigPath); os.IsNotExist(err) {
		resp.Diagnostics.AddError(
			"Import Failed: Kubeconfig File Not Found",
			fmt.Sprintf("Kubeconfig file not found at: %s\n\n"+
				"Ensure your kubeconfig file exists or set KUBECONFIG environment variable:\n"+
				"  export KUBECONFIG=/path/to/your/kubeconfig\n"+
				"  terraform import k8sinline_manifest.example \"prod/default/Pod/nginx\"", kubeconfigPath),
		)
		return
	}

	// Create K8s client using kubeconfig file and context
	client, err := k8sclient.NewDynamicK8sClientFromKubeconfigFile(kubeconfigPath, kubeContext)
	if err != nil {
		// Provide context-specific error messages
		if strings.Contains(err.Error(), "context") && strings.Contains(err.Error(), "not found") {
			resp.Diagnostics.AddError(
				"Import Failed: Context Not Found",
				fmt.Sprintf("Context \"%s\" not found in kubeconfig.\n\n"+
					"Available contexts:\n"+
					"  kubectl config get-contexts\n\n"+
					"Details: %s", kubeContext, err.Error()),
			)
		} else if strings.Contains(err.Error(), "kubeconfig") {
			resp.Diagnostics.AddError(
				"Import Failed: Invalid Kubeconfig",
				fmt.Sprintf("Failed to parse kubeconfig file at %s.\n\n"+
					"Ensure your kubeconfig is valid:\n"+
					"  kubectl config view\n\n"+
					"Details: %s", kubeconfigPath, err.Error()),
			)
		} else {
			resp.Diagnostics.AddError(
				"Import Failed: Connection Error",
				fmt.Sprintf("Failed to create Kubernetes client from kubeconfig.\n\n"+
					"This usually means:\n"+
					"1. Invalid kubeconfig file\n"+
					"2. Cluster is unreachable\n"+
					"3. Authentication failed\n\n"+
					"Kubeconfig: %s\n"+
					"Context: %s\n"+
					"Details: %s", kubeconfigPath, kubeContext, err.Error()),
			)
		}
		return
	}

	// Discover GVR and fetch the live object in one step
	_, liveObj, err := client.GetGVRFromKind(ctx, kind, namespace, name)
	if err != nil {
		if strings.Contains(err.Error(), "no API resource found for kind") {
			resp.Diagnostics.AddError(
				"Import Failed: Unknown Resource Kind",
				fmt.Sprintf("The resource kind \"%s\" was not found in the cluster.\n\n"+
					"This usually means:\n"+
					"1. The kind name is misspelled (check capitalization)\n"+
					"2. A CRD needs to be installed first\n"+
					"3. The resource type doesn't exist in this Kubernetes version\n\n"+
					"Check available resource types:\n"+
					"  kubectl api-resources | grep -i %s", kind, strings.ToLower(kind)),
			)
		} else if strings.Contains(err.Error(), "not found") {
			resp.Diagnostics.AddError(
				"Import Failed: Resource Not Found",
				fmt.Sprintf("The %s \"%s\" was not found in the cluster.\n\n"+
					"Verify the resource exists:\n"+
					"  kubectl get %s %s %s\n\n"+
					"Context: %s\n"+
					"Details: %s",
					kind, name, strings.ToLower(kind), name,
					func() string {
						if namespace != "" {
							return fmt.Sprintf("-n %s", namespace)
						}
						return ""
					}(), kubeContext, err.Error()),
			)
		} else {
			resp.Diagnostics.AddError(
				"Import Failed: Discovery/Fetch Error",
				fmt.Sprintf("Failed to discover or fetch the %s \"%s\".\n\n"+
					"Context: %s\n"+
					"Details: %s", kind, name, kubeContext, err.Error()),
			)
		}
		return
	}

	// Convert live object back to clean YAML
	yamlBytes, err := r.objectToYAML(liveObj)
	if err != nil {
		resp.Diagnostics.AddError(
			"Import Failed: YAML Conversion Error",
			fmt.Sprintf("Failed to convert the imported object to YAML: %s", err.Error()),
		)
		return
	}

	// Generate resource ID using a special import-based approach
	// Since we don't have the final cluster connection yet, we'll use the context
	resourceID := r.generateIDFromImport(liveObj, kubeContext)

	// Create connection model for import
	connModel := ClusterConnectionModel{
		Host:                 types.StringNull(),
		ClusterCACertificate: types.StringNull(),
		KubeconfigFile:       types.StringValue(kubeconfigPath),
		KubeconfigRaw:        types.StringNull(),
		Context:              types.StringValue(kubeContext),
		Exec:                 nil,
	}

	// Convert to types.Object
	connObj, err := r.convertConnectionModelToObject(ctx, connModel)
	if err != nil {
		resp.Diagnostics.AddError(
			"Import Failed: Connection Conversion Error",
			fmt.Sprintf("Failed to convert connection model to object: %s", err.Error()),
		)
		return
	}

	// Populate state with imported data
	importedData := manifestResourceModel{
		ID:                types.StringValue(resourceID),
		YAMLBody:          types.StringValue(string(yamlBytes)),
		ClusterConnection: connObj,                // Now using types.Object
		DeleteProtection:  types.BoolValue(false), // default
	}

	// Set the imported state
	diags := resp.State.Set(ctx, &importedData)
	resp.Diagnostics.Append(diags...)
	if resp.Diagnostics.HasError() {
		return
	}

	tflog.Info(ctx, "successfully imported resource", map[string]interface{}{
		"import_id":   req.ID,
		"resource_id": resourceID,
		"kind":        liveObj.GetKind(),
		"name":        liveObj.GetName(),
		"namespace":   liveObj.GetNamespace(),
		"context":     kubeContext,
		"kubeconfig":  kubeconfigPath,
	})

	// Add informational message about next steps
	resp.Diagnostics.AddWarning(
		"Import Successful - Configuration Required",
		"The resource has been imported successfully. You must now configure the cluster_connection block in your Terraform configuration to match your desired connection method.\n\n"+
			"Example configuration:\n"+
			"  resource \"k8sinline_manifest\" \"example\" {\n"+
			"    yaml_body = \"# Populated by import\"\n"+
			"    \n"+
			"    cluster_connection = {\n"+
			"      # Choose your preferred connection method:\n"+
			"      kubeconfig_file = \"~/.kube/config\"\n"+
			"      context         = \""+kubeContext+"\"\n"+
			"    }\n"+
			"  }\n\n"+
			"Run 'terraform plan' to see if your configuration matches the imported resource.",
	)
}

// parseYAML converts YAML string to unstructured.Unstructured
func (r *manifestResource) parseYAML(yamlStr string) (*unstructured.Unstructured, error) {
	obj := &unstructured.Unstructured{}
	err := yaml.Unmarshal([]byte(yamlStr), obj)
	if err != nil {
		return nil, fmt.Errorf("failed to unmarshal YAML: %w", err)
	}

	// Validate required fields
	if obj.GetAPIVersion() == "" {
		return nil, fmt.Errorf("apiVersion is required")
	}
	if obj.GetKind() == "" {
		return nil, fmt.Errorf("kind is required")
	}
	if obj.GetName() == "" {
		return nil, fmt.Errorf("metadata.name is required")
	}

	return obj, nil
}

// createK8sClient creates a K8sClient from cluster connection configuration
func (r *manifestResource) createK8sClient(conn ClusterConnectionModel) (k8sclient.K8sClient, error) {
	// Determine connection mode
	hasInline := !conn.Host.IsNull() || !conn.ClusterCACertificate.IsNull()
	hasFile := !conn.KubeconfigFile.IsNull()
	hasRaw := !conn.KubeconfigRaw.IsNull()

	modeCount := 0
	if hasInline {
		modeCount++
	}
	if hasFile {
		modeCount++
	}
	if hasRaw {
		modeCount++
	}

	if modeCount == 0 {
		return nil, fmt.Errorf("must specify exactly one of: inline connection, kubeconfig_file, or kubeconfig_raw")
	}
	if modeCount > 1 {
		return nil, fmt.Errorf("cannot specify multiple connection modes")
	}

	// Create REST config
	var config *rest.Config
	var err error

	switch {
	case hasInline:
		config, err = r.createInlineConfig(conn)
	case hasFile:
		config, err = r.createFileConfig(conn)
	case hasRaw:
		config, err = r.createRawConfig(conn)
	default:
		return nil, fmt.Errorf("no valid connection mode specified")
	}

	if err != nil {
		return nil, err
	}

	// Use simple dynamic client
	return k8sclient.NewDynamicK8sClient(config)
}

// createInlineConfig creates a REST config from inline connection settings
func (r *manifestResource) createInlineConfig(conn ClusterConnectionModel) (*rest.Config, error) {
	if conn.Host.IsNull() {
		return nil, fmt.Errorf("host is required for inline connection")
	}
	if conn.ClusterCACertificate.IsNull() {
		return nil, fmt.Errorf("cluster_ca_certificate is required for inline connection")
	}

	// Decode base64-encoded CA certificate
	caData, err := base64.StdEncoding.DecodeString(conn.ClusterCACertificate.ValueString())
	if err != nil {
		return nil, fmt.Errorf("failed to decode cluster_ca_certificate: %w", err)
	}

	// Build REST config directly
	config := &rest.Config{
		Host: conn.Host.ValueString(),
		TLSClientConfig: rest.TLSClientConfig{
			CAData: caData,
		},
	}

	// Add exec provider if specified
	if conn.Exec != nil && !conn.Exec.APIVersion.IsNull() {
		args := make([]string, len(conn.Exec.Args))
		for i, arg := range conn.Exec.Args {
			args[i] = arg.ValueString()
		}

		config.ExecProvider = &clientcmdapi.ExecConfig{
			APIVersion:      conn.Exec.APIVersion.ValueString(),
			Command:         conn.Exec.Command.ValueString(),
			Args:            args,
			Env:             []clientcmdapi.ExecEnvVar{},
			InteractiveMode: clientcmdapi.NeverExecInteractiveMode,
		}
	}

	return config, nil
}

// createFileConfig creates a REST config from kubeconfig file
func (r *manifestResource) createFileConfig(conn ClusterConnectionModel) (*rest.Config, error) {
	kubeconfigPath := conn.KubeconfigFile.ValueString()
	context := ""
	if !conn.Context.IsNull() {
		context = conn.Context.ValueString()
	}

	if context != "" {
		// Load kubeconfig file and set context
		clientConfig := clientcmd.NewNonInteractiveDeferredLoadingClientConfig(
			&clientcmd.ClientConfigLoadingRules{ExplicitPath: kubeconfigPath},
			&clientcmd.ConfigOverrides{CurrentContext: context},
		)
		return clientConfig.ClientConfig()
	}

	return clientcmd.BuildConfigFromFlags("", kubeconfigPath)
}

// createRawConfig creates a REST config from raw kubeconfig data
func (r *manifestResource) createRawConfig(conn ClusterConnectionModel) (*rest.Config, error) {
	kubeconfigData := []byte(conn.KubeconfigRaw.ValueString())

	config, err := clientcmd.RESTConfigFromKubeConfig(kubeconfigData)
	if err != nil {
		return nil, fmt.Errorf("failed to parse kubeconfig: %w", err)
	}

	if !conn.Context.IsNull() {
		context := conn.Context.ValueString()
		// Load kubeconfig and set context
		clientConfig, err := clientcmd.Load(kubeconfigData)
		if err != nil {
			return nil, fmt.Errorf("failed to load kubeconfig: %w", err)
		}

		if _, exists := clientConfig.Contexts[context]; !exists {
			return nil, fmt.Errorf("context %q not found in kubeconfig", context)
		}

		clientConfig.CurrentContext = context
		return clientcmd.NewDefaultClientConfig(*clientConfig, &clientcmd.ConfigOverrides{}).ClientConfig()
	}

	return config, nil
}

// getGVR determines the GroupVersionResource for an object
func (r *manifestResource) getGVR(ctx context.Context, client k8sclient.K8sClient, obj *unstructured.Unstructured) (k8sschema.GroupVersionResource, error) {
	return client.GetGVR(ctx, obj)
}

// generateID creates a unique identifier for the resource
func (r *manifestResource) generateID(obj *unstructured.Unstructured, conn ClusterConnectionModel) string {
	// Create a deterministic ID based on cluster + object identity
	data := fmt.Sprintf("%s/%s/%s/%s",
		r.getClusterID(conn),
		obj.GetNamespace(),
		obj.GetKind(),
		obj.GetName(),
	)

	hash := sha256.Sum256([]byte(data))
	return hex.EncodeToString(hash[:])
}

// Helper function to generateID after import:
func (r *manifestResource) generateIDFromImport(obj *unstructured.Unstructured, context string) string {
	data := fmt.Sprintf("%s/%s/%s/%s",
		context, // Use context as cluster identifier for imports
		obj.GetNamespace(),
		obj.GetKind(),
		obj.GetName(),
	)

	hash := sha256.Sum256([]byte(data))
	return hex.EncodeToString(hash[:])
}

// getClusterID creates a stable identifier for the cluster connection
func (r *manifestResource) getClusterID(conn ClusterConnectionModel) string {
	// Use host if available, otherwise hash the kubeconfig
	if !conn.Host.IsNull() {
		return conn.Host.ValueString()
	}

	var data string
	if !conn.KubeconfigFile.IsNull() {
		data = conn.KubeconfigFile.ValueString()
	} else if !conn.KubeconfigRaw.IsNull() {
		data = conn.KubeconfigRaw.ValueString()
	}

	hash := sha256.Sum256([]byte(data))
	return hex.EncodeToString(hash[:8]) // Use first 8 bytes for shorter ID
}

// classifyK8sError categorizes Kubernetes API errors for better user experience
func (r *manifestResource) classifyK8sError(err error, operation, resourceDesc string) (severity, title, detail string) {
	switch {
	case errors.IsNotFound(err):
		return "warning", fmt.Sprintf("%s: Resource Not Found", operation),
			fmt.Sprintf("The %s was not found in the cluster. It may have been deleted outside of Terraform.", resourceDesc)

	case errors.IsForbidden(err):
		return "error", fmt.Sprintf("%s: Insufficient Permissions", operation),
			fmt.Sprintf("RBAC permissions insufficient to %s %s. Check that your credentials have the required permissions for this operation. Details: %v",
				operation, resourceDesc, err)

	case errors.IsConflict(err):
		return "error", fmt.Sprintf("%s: Field Manager Conflict", operation),
			fmt.Sprintf("Server-side apply conflict detected for %s. Another tool or process may be managing the same fields. Consider using 'force=true' or resolve the conflict manually. Details: %v",
				resourceDesc, err)

	case errors.IsTimeout(err) || errors.IsServerTimeout(err):
		return "error", fmt.Sprintf("%s: Kubernetes API Timeout", operation),
			fmt.Sprintf("Timeout while performing %s on %s. The cluster may be under heavy load or experiencing connectivity issues. Details: %v",
				operation, resourceDesc, err)

	case errors.IsUnauthorized(err):
		return "error", fmt.Sprintf("%s: Authentication Failed", operation),
			fmt.Sprintf("Authentication failed for %s %s. Check your credentials and ensure they are valid. Details: %v",
				operation, resourceDesc, err)

	case errors.IsInvalid(err):
		return "error", fmt.Sprintf("%s: Invalid Resource", operation),
			fmt.Sprintf("The %s contains invalid fields or values. Review the YAML specification and ensure all required fields are present and correctly formatted. Details: %v",
				resourceDesc, err)

	case errors.IsAlreadyExists(err):
		return "error", fmt.Sprintf("%s: Resource Already Exists", operation),
			fmt.Sprintf("The %s already exists in the cluster and cannot be created. Use import to manage existing resources with Terraform. Details: %v",
				resourceDesc, err)

	default:
		return "error", fmt.Sprintf("%s: Kubernetes API Error", operation),
			fmt.Sprintf("An unexpected error occurred while performing %s on %s. Details: %v",
				operation, resourceDesc, err)
	}
}

// Updated parseImportID function to handle new format with context
func (r *manifestResource) parseImportID(importID string) (context, namespace, kind, name string, err error) {
	parts := strings.Split(importID, "/")

	switch len(parts) {
	case 3:
		// Cluster-scoped: "context/kind/name"
		return parts[0], "", parts[1], parts[2], nil
	case 4:
		// Namespaced: "context/namespace/kind/name"
		return parts[0], parts[1], parts[2], parts[3], nil
	default:
		return "", "", "", "", fmt.Errorf("expected 3 or 4 parts separated by '/', got %d parts", len(parts))
	}
}

// isEmptyConnection checks if the cluster connection is empty/unconfigured
func (r *manifestResource) isEmptyConnection(conn ClusterConnectionModel) bool {
	hasInline := !conn.Host.IsNull() || !conn.ClusterCACertificate.IsNull()
	hasFile := !conn.KubeconfigFile.IsNull()
	hasRaw := !conn.KubeconfigRaw.IsNull()

	return !hasInline && !hasFile && !hasRaw
}

// objectToYAML converts an unstructured object back to clean YAML
func (r *manifestResource) objectToYAML(obj *unstructured.Unstructured) ([]byte, error) {
	// Create a clean copy without managed fields and other cluster-added metadata
	cleanObj := r.cleanObjectForExport(obj)

	// Convert to YAML
	yamlBytes, err := yaml.Marshal(cleanObj.Object)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal object to YAML: %w", err)
	}

	return yamlBytes, nil
}

// cleanObjectForExport removes server-generated fields that would cause apply failures
func (r *manifestResource) cleanObjectForExport(obj *unstructured.Unstructured) *unstructured.Unstructured {
	// Create a deep copy
	cleaned := obj.DeepCopy()

	// Remove only the fields that will definitely cause problems on re-apply
	metadata := cleaned.Object["metadata"].(map[string]interface{})

	// These fields MUST be removed or kubectl apply fails
	delete(metadata, "uid")
	delete(metadata, "resourceVersion")
	delete(metadata, "generation")
	delete(metadata, "creationTimestamp")
	delete(metadata, "managedFields")

	// Remove status field entirely (never needed for apply)
	delete(cleaned.Object, "status")

	// Leave everything else - let the user clean up if they want
	// This is safer than trying to guess what's system-generated

	return cleaned
}

func (r *manifestResource) validateOwnership(ctx context.Context, data manifestResourceModel) error {
	obj, err := r.parseYAML(data.YAMLBody.ValueString())
	if err != nil {
		return fmt.Errorf("failed to parse YAML: %w", err)
	}

	// Convert connection object to model
	conn, err := r.convertObjectToConnectionModel(ctx, data.ClusterConnection)
	if err != nil {
		return fmt.Errorf("failed to convert connection: %w", err)
	}

	client, err := r.clientGetter(conn)
	if err != nil {
		return fmt.Errorf("failed to create client: %w", err)
	}

	gvr, err := r.getGVR(ctx, client, obj)
	if err != nil {
		return fmt.Errorf("failed to determine GVR: %w", err)
	}

	liveObj, err := client.Get(ctx, gvr, obj.GetNamespace(), obj.GetName())
	if err != nil {
		if errors.IsNotFound(err) {
			return nil // Object doesn't exist - safe
		}
		return fmt.Errorf("failed to check existing resource: %w", err)
	}

	// Check ownership annotation
	annotations := liveObj.GetAnnotations()
	if annotations == nil {
		return fmt.Errorf("resource exists but has no ownership annotation - may be unmanaged")
	}

	actualID := annotations["k8sinline.terraform.io/id"]
	expectedID := data.ID.ValueString()

	if actualID != expectedID {
		return fmt.Errorf("connection targets different cluster - resource %s %q exists but is not managed by this Terraform resource (different ID: %s vs %s)",
			obj.GetKind(), obj.GetName(), actualID, expectedID)
	}

	return nil // Same resource, safe to proceed
}

func (r *manifestResource) anyConnectionFieldChanged(plan, state ClusterConnectionModel) bool {
	return !plan.Host.Equal(state.Host) ||
		!plan.ClusterCACertificate.Equal(state.ClusterCACertificate) ||
		!plan.KubeconfigFile.Equal(state.KubeconfigFile) ||
		!plan.KubeconfigRaw.Equal(state.KubeconfigRaw) ||
		!plan.Context.Equal(state.Context) ||
		!reflect.DeepEqual(plan.Exec, state.Exec)
}

// forceDestroy removes finalizers and forces deletion
func (r *manifestResource) forceDestroy(ctx context.Context, client k8sclient.K8sClient, gvr k8sschema.GroupVersionResource, obj *unstructured.Unstructured, resp *resource.DeleteResponse) error {
	// Get the current state of the object
	liveObj, err := client.Get(ctx, gvr, obj.GetNamespace(), obj.GetName())
	if err != nil {
		if errors.IsNotFound(err) {
			// Object disappeared on its own - that's fine
			return nil
		}
		return fmt.Errorf("failed to get object for force destroy: %w", err)
	}

	// Check if object has finalizers
	finalizers := liveObj.GetFinalizers()
	if len(finalizers) == 0 {
		// No finalizers, but still stuck - this is unusual
		tflog.Warn(ctx, "Object has no finalizers but deletion timed out", map[string]interface{}{
			"resource": fmt.Sprintf("%s/%s", obj.GetKind(), obj.GetName()),
		})

		// Try deleting again in case it was a timing issue
		err = client.Delete(ctx, gvr, obj.GetNamespace(), obj.GetName(), k8sclient.DeleteOptions{})
		if err != nil && !errors.IsNotFound(err) {
			return fmt.Errorf("failed to re-delete object without finalizers: %w", err)
		}

		// Wait a bit more for the deletion
		return r.waitForDeletion(ctx, client, gvr, obj, 30*time.Second)
	}

	// Log what finalizers we're about to remove
	resp.Diagnostics.AddWarning(
		"Force Destroying Resource with Finalizers",
		fmt.Sprintf("Removing finalizers from %s %s to force deletion: %v\n\n"+
			"⚠️  WARNING: This bypasses Kubernetes safety mechanisms and may cause:\n"+
			"• Data loss or corruption\n"+
			"• Orphaned dependent resources\n"+
			"• Incomplete cleanup operations\n\n"+
			"Only use force_destroy when you understand the implications for your specific resource.",
			obj.GetKind(), obj.GetName(), finalizers),
	)

	// Remove all finalizers
	liveObj.SetFinalizers([]string{})

	// Apply the change (remove finalizers)
	err = client.Apply(ctx, liveObj, k8sclient.ApplyOptions{
		FieldManager: "k8sinline-force-destroy",
		Force:        true,
	})
	if err != nil {
		return fmt.Errorf("failed to remove finalizers: %w", err)
	}

	// Wait for deletion to complete (should be quick now)
	return r.waitForDeletion(ctx, client, gvr, obj, 60*time.Second)
}

// handleDeletionTimeout provides helpful guidance when normal deletion times out
func (r *manifestResource) handleDeletionTimeout(resp *resource.DeleteResponse, client k8sclient.K8sClient, gvr k8sschema.GroupVersionResource, obj *unstructured.Unstructured, timeout time.Duration, timeoutErr error) {
	ctx := context.Background()

	// Try to get current state to see what's preventing deletion
	liveObj, err := client.Get(ctx, gvr, obj.GetNamespace(), obj.GetName())
	if err != nil {
		if errors.IsNotFound(err) {
			// Object disappeared between timeout and this check
			tflog.Info(ctx, "Object deleted after timeout check")
			return
		}

		// Can't get object state, provide generic timeout error
		resp.Diagnostics.AddError(
			"Deletion Timeout",
			fmt.Sprintf("Resource %s %s could not be deleted within %v.\n\n"+
				"The resource may still be terminating in the background. "+
				"Check its status with: kubectl get %s %s %s\n\n"+
				"To force deletion (⚠️ may cause data loss), set force_destroy = true",
				obj.GetKind(), obj.GetName(), timeout,
				strings.ToLower(obj.GetKind()), obj.GetName(), r.namespaceFlag(obj)),
		)
		return
	}

	// Check if finalizers are preventing deletion
	finalizers := liveObj.GetFinalizers()
	deletionTimestamp := liveObj.GetDeletionTimestamp()

	if deletionTimestamp != nil && len(finalizers) > 0 {
		// Object is terminating but blocked by finalizers
		resp.Diagnostics.AddError(
			"Deletion Blocked by Finalizers",
			fmt.Sprintf("Resource %s %s has been marked for deletion but is blocked by finalizers: %v\n\n"+
				"Finalizers prevent deletion until cleanup operations complete. Options:\n\n"+
				"1. **Wait longer** - increase delete_timeout (some operations take time):\n"+
				"   delete_timeout = \"20m\"\n\n"+
				"2. **Force deletion** - bypass finalizers (⚠️ may cause data loss):\n"+
				"   force_destroy = true\n\n"+
				"3. **Manual intervention** - check what's preventing cleanup:\n"+
				"   kubectl describe %s %s %s\n"+
				"   kubectl get events --field-selector involvedObject.name=%s\n\n"+
				"4. **Remove finalizers manually** (⚠️ dangerous):\n"+
				"   kubectl patch %s %s %s --type='merge' -p '{\"metadata\":{\"finalizers\":null}}'",
				obj.GetKind(), obj.GetName(), finalizers,
				strings.ToLower(obj.GetKind()), obj.GetName(), r.namespaceFlag(obj), obj.GetName(),
				strings.ToLower(obj.GetKind()), obj.GetName(), r.namespaceFlag(obj)),
		)
	} else if deletionTimestamp != nil {
		// Object is terminating but no finalizers - something else is wrong
		resp.Diagnostics.AddError(
			"Deletion Stuck Without Finalizers",
			fmt.Sprintf("Resource %s %s has been marked for deletion but is not terminating normally.\n\n"+
				"This may indicate a cluster issue. Check:\n"+
				"• kubectl describe %s %s %s\n"+
				"• kubectl get events --field-selector involvedObject.name=%s\n"+
				"• Cluster controller logs\n\n"+
				"To force deletion anyway: set force_destroy = true",
				obj.GetKind(), obj.GetName(),
				strings.ToLower(obj.GetKind()), obj.GetName(), r.namespaceFlag(obj), obj.GetName()),
		)
	} else {
		// Object exists but no deletion timestamp - delete call may have failed silently
		resp.Diagnostics.AddError(
			"Deletion Not Initiated",
			fmt.Sprintf("Resource %s %s still exists and has not been marked for deletion.\n\n"+
				"This may indicate insufficient permissions or a cluster issue.\n"+
				"Check: kubectl describe %s %s %s\n\n"+
				"To force deletion: set force_destroy = true",
				obj.GetKind(), obj.GetName(),
				strings.ToLower(obj.GetKind()), obj.GetName(), r.namespaceFlag(obj)),
		)
	}
}

// getDeleteTimeout determines the appropriate timeout for resource deletion
func (r *manifestResource) getDeleteTimeout(data manifestResourceModel) time.Duration {
	// If user specified a timeout, use it
	if !data.DeleteTimeout.IsNull() {
		if timeout, err := time.ParseDuration(data.DeleteTimeout.ValueString()); err == nil {
			return timeout
		}
	}

	// Parse YAML to determine resource type for default timeout
	if obj, err := r.parseYAML(data.YAMLBody.ValueString()); err == nil {
		kind := obj.GetKind()

		// Set default timeouts based on resource type
		switch kind {
		case "Namespace", "PersistentVolume", "PersistentVolumeClaim":
			return 10 * time.Minute // Resources that often have finalizers
		case "CustomResourceDefinition":
			return 15 * time.Minute // CRDs need extra time for controller cleanup
		case "StatefulSet", "Job", "CronJob":
			return 8 * time.Minute // Ordered deletion or foreground deletion
		default:
			return 5 * time.Minute // Default for most resources
		}
	}

	// Fallback if YAML parsing fails
	return 5 * time.Minute
}

// waitForDeletion waits for a resource to be deleted from the cluster
func (r *manifestResource) waitForDeletion(ctx context.Context, client k8sclient.K8sClient, gvr k8sschema.GroupVersionResource, obj *unstructured.Unstructured, timeout time.Duration, ignoreFinalizers ...bool) error {
	// If timeout is 0, skip waiting
	if timeout == 0 {
		return nil
	}

	ignoreFinalizersFlag := false
	if len(ignoreFinalizers) > 0 {
		ignoreFinalizersFlag = ignoreFinalizers[0]
	}

	// Use a ticker to poll periodically instead of tight loop
	ticker := time.NewTicker(2 * time.Second)
	defer ticker.Stop()

	deadline := time.Now().Add(timeout)

	for {
		select {
		case <-ctx.Done():
			return ctx.Err()
		case <-ticker.C:
			// Check if object still exists
			_, err := client.Get(ctx, gvr, obj.GetNamespace(), obj.GetName())
			if err != nil {
				if errors.IsNotFound(err) {
					// Successfully deleted
					return nil
				}
				// Other errors are not deletion success, continue waiting
			}

			// Check if we've exceeded the timeout
			if time.Now().After(deadline) {
				if ignoreFinalizersFlag {
					// When ignoring finalizers, don't treat timeout as error
					return nil
				}
				return fmt.Errorf("timeout after %v waiting for deletion", timeout)
			}
		}
	}
}

// namespaceFlag returns the kubectl namespace flag for the given object
func (r *manifestResource) namespaceFlag(obj *unstructured.Unstructured) string {
	if namespace := obj.GetNamespace(); namespace != "" {
		return fmt.Sprintf("-n %s", namespace)
	}
	return ""
}

// convertObjectToConnectionModel converts types.Object to ClusterConnectionModel
func (r *manifestResource) convertObjectToConnectionModel(ctx context.Context, obj types.Object) (ClusterConnectionModel, error) {
	if obj.IsNull() {
		return ClusterConnectionModel{}, fmt.Errorf("cluster connection is null")
	}

	if obj.IsUnknown() {
		return ClusterConnectionModel{}, fmt.Errorf("cluster connection contains unknown values")
	}

	var conn ClusterConnectionModel
	diags := obj.As(ctx, &conn, basetypes.ObjectAsOptions{})
	if diags.HasError() {
		return ClusterConnectionModel{}, fmt.Errorf("failed to convert cluster connection: %s", diags)
	}

	return conn, nil
}

// isConnectionReady checks if the connection object is ready (not null/unknown)
func (r *manifestResource) isConnectionReady(obj types.Object) bool {
	return !obj.IsNull() && !obj.IsUnknown()
}

// convertConnectionModelToObject converts ClusterConnectionModel to types.Object
func (r *manifestResource) convertConnectionModelToObject(ctx context.Context, conn ClusterConnectionModel) (types.Object, error) {
	// Define the object type based on our schema
	objectType := types.ObjectType{
		AttrTypes: map[string]attr.Type{
			"host":                   types.StringType,
			"cluster_ca_certificate": types.StringType,
			"kubeconfig_file":        types.StringType,
			"kubeconfig_raw":         types.StringType,
			"context":                types.StringType,
			"exec": types.ObjectType{
				AttrTypes: map[string]attr.Type{
					"api_version": types.StringType,
					"command":     types.StringType,
					"args":        types.ListType{ElemType: types.StringType},
				},
			},
		},
	}

	obj, diags := types.ObjectValueFrom(ctx, objectType.AttrTypes, conn)
	if diags.HasError() {
		return types.ObjectNull(objectType.AttrTypes), fmt.Errorf("failed to convert connection model to object: %s", diags)
	}

	return obj, nil
}
