---
page_title: "Resource k8sconnect_object - terraform-provider-k8sconnect"
subcategory: ""
description: |-
  Applies a single‑document Kubernetes YAML manifest to a cluster, with per‑resource inline or kubeconfig‑based connection settings.
---

# Resource: k8sconnect_object

Applies a single‑document Kubernetes YAML manifest to a cluster, with per‑resource inline or kubeconfig‑based connection settings.

## Example Usage - Bootstrap EKS Cluster with Workloads

Single terraform apply to create cluster and deploy workloads:

```terraform
provider "k8sconnect" {}  # No configuration needed

# Create a new cluster
resource "aws_eks_cluster" "main" {
  name     = "my-cluster"
  # ... (cluster configuration)
}

resource "aws_eks_node_group" "main" {
  cluster_name = aws_eks_cluster.main.name
  # ... (node group configuration)
}

# Connection can be reused across resources
locals {
  cluster_connection = {
    host                   = aws_eks_cluster.main.endpoint
    cluster_ca_certificate = aws_eks_cluster.main.certificate_authority[0].data
    exec = {
      api_version = "client.authentication.k8s.io/v1"
      command     = "aws"
      args        = ["eks", "get-token", "--cluster-name", aws_eks_cluster.main.name]
    }
  }
}

# Deploy workloads immediately - no waiting for provider configuration!
resource "k8sconnect_object" "app" {
  yaml_body          = file("deployment.yaml")
  cluster_connection = local.cluster_connection

  # For EKS: ensure nodes are ready before deploying workloads
  depends_on = [aws_eks_node_group.main]
}
```

## Example Usage - Basic Deployment

<!-- runnable-test: object-basic-deployment -->
```terraform
resource "k8sconnect_object" "namespace" {
  yaml_body = <<-YAML
    apiVersion: v1
    kind: Namespace
    metadata:
      name: example
  YAML

  cluster_connection = var.cluster_connection
}

resource "k8sconnect_object" "deployment" {
  yaml_body = <<-YAML
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: nginx
      namespace: example
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: nginx:1.21
  YAML

  cluster_connection = var.cluster_connection
  depends_on         = [k8sconnect_object.namespace]
}
```
<!-- /runnable-test -->

## Example Usage - Coexisting with HPA

Use `ignore_fields` to let the HorizontalPodAutoscaler manage replicas:

<!-- runnable-test: object-hpa-coexistence -->
```terraform
resource "k8sconnect_object" "app" {
  yaml_body = <<-YAML
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: nginx-with-hpa
      namespace: example
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: nginx:1.21
            resources:
              requests:
                cpu: 100m
  YAML

  # Ignore spec.replicas because HPA will modify it
  ignore_fields = ["spec.replicas"]

  cluster_connection = var.cluster_connection
}

resource "k8sconnect_object" "hpa" {
  yaml_body = <<-YAML
    apiVersion: autoscaling/v2
    kind: HorizontalPodAutoscaler
    metadata:
      name: nginx-hpa
      namespace: example
    spec:
      scaleTargetRef:
        apiVersion: apps/v1
        kind: Deployment
        name: nginx-with-hpa
      minReplicas: 1
      maxReplicas: 10
      metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 50
  YAML

  cluster_connection = var.cluster_connection
  depends_on         = [k8sconnect_object.app]
}
```
<!-- /runnable-test -->

## Example Usage - Multi-Cluster Deployment

Deploy the same resource to multiple clusters:

```terraform
locals {
  prod_connection = {
    host                   = aws_eks_cluster.prod.endpoint
    cluster_ca_certificate = base64decode(aws_eks_cluster.prod.certificate_authority[0].data)
    exec = {
      api_version = "client.authentication.k8s.io/v1"
      command     = "aws"
      args        = ["eks", "get-token", "--cluster-name", "prod"]
    }
  }

  staging_connection = {
    kubeconfig = file("~/.kube/staging-config")
    context    = "staging"
  }
}

resource "k8sconnect_object" "prod_app" {
  yaml_body          = file("app.yaml")
  cluster_connection = local.prod_connection
}

resource "k8sconnect_object" "staging_app" {
  yaml_body          = file("app.yaml")
  cluster_connection = local.staging_connection
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `cluster_connection` (Attributes) Kubernetes cluster connection for this specific resource. Can be different per-resource, enabling multi-cluster deployments without provider aliases. Supports inline credentials (token, exec, client certs) or kubeconfig. (see [below for nested schema](#nestedatt--cluster_connection))
- `yaml_body` (String) UTF-8 encoded, single-document Kubernetes YAML. Multi-doc files will fail validation.

### Optional

- `delete_protection` (Boolean) Prevent accidental deletion of the resource. If set to true, the resource cannot be deleted unless this field is set to false.
- `delete_timeout` (String) How long to wait for a resource to be deleted before considering the deletion failed. Defaults to 300s (5 minutes).
- `force_destroy` (Boolean) Force deletion by removing finalizers. **WARNING:** Unlike other providers, this REMOVES finalizers after timeout. May cause data loss and orphaned cloud resources. Consult documentation before enabling.
- `ignore_fields` (List of String) Field paths to exclude from management using JSONPath syntax. Use for fields controlled by other systems (HPA replicas, cert-manager CA bundles, operator annotations). Supports dot notation ('spec.replicas'), positional arrays ('webhooks[0].caBundle'), and JSONPath predicates ('containers[?(@.name=="nginx")].image'). Example: 'spec.template.spec.containers[?(@.name=="app")].env[?(@.name=="EXTERNAL_VAR")].value'

### Read-Only

- `field_ownership` (Map of String) Tracks which controller owns each managed field using Server-Side Apply field management. Shows as a map of 'field.path': 'controller-name'. Only appears in plan diffs when ownership actually changes (e.g., when HPA takes ownership of spec.replicas). Empty/hidden when ownership is unchanged. Critical for understanding SSA conflicts and knowing which controller controls what.
- `id` (String) Unique identifier for this manifest (generated by the provider).
- `managed_state_projection` (Map of String) Field-by-field snapshot of managed state as flat key-value pairs with dotted paths. Shows exactly which fields k8sconnect manages and their current values. Terraform automatically displays only changed keys in diffs for clean, scannable output. When this differs from current cluster state, it indicates drift - someone modified your managed fields outside Terraform. Computed via Server-Side Apply dry-run for accuracy, enabling precise drift detection without false positives.
- `object_ref` (Attributes) Kubernetes object reference containing the identity of the applied resource. Populated after successful apply. Used by k8sconnect_wait resource to locate the object for waiting. Contains api_version, kind, name, and namespace (if namespaced). (see [below for nested schema](#nestedatt--object_ref))

<a id="nestedatt--cluster_connection"></a>
### Nested Schema for `cluster_connection`

Optional:

- `client_certificate` (String, Sensitive) Client certificate for TLS authentication. Accepts PEM format or base64-encoded PEM - automatically detected.
- `client_key` (String, Sensitive) Client certificate key for TLS authentication. Accepts PEM format or base64-encoded PEM - automatically detected.
- `cluster_ca_certificate` (String, Sensitive) Root certificate bundle for TLS authentication. Accepts PEM format or base64-encoded PEM - automatically detected.
- `context` (String) Context to use from the kubeconfig. Optional when kubeconfig contains exactly one context (that context will be used automatically). Required when kubeconfig contains multiple contexts to prevent accidental connection to the wrong cluster. Error will list available contexts if not specified when required.
- `exec` (Attributes, Sensitive) Configuration for exec-based authentication. (see [below for nested schema](#nestedatt--cluster_connection--exec))
- `host` (String) The hostname (in form of URI) of the Kubernetes API server.
- `insecure` (Boolean) Whether server should be accessed without verifying the TLS certificate.
- `kubeconfig` (String, Sensitive) Raw kubeconfig file content.
- `proxy_url` (String) URL of the proxy to use for requests.
- `token` (String, Sensitive) Token to authenticate to the Kubernetes API server.

<a id="nestedatt--cluster_connection--exec"></a>
### Nested Schema for `cluster_connection.exec`

Required:

- `api_version` (String) API version to use when encoding the ExecCredentials resource.
- `command` (String) Command to execute.

Optional:

- `args` (List of String) Arguments to pass when executing the plugin.
- `env` (Map of String) Environment variables to set when executing the plugin.



<a id="nestedatt--object_ref"></a>
### Nested Schema for `object_ref`

Read-Only:

- `api_version` (String) Kubernetes API version (e.g., 'v1', 'apps/v1')
- `kind` (String) Kubernetes resource kind (e.g., 'Pod', 'Deployment')
- `name` (String) Resource name from metadata.name
- `namespace` (String) Resource namespace from metadata.namespace. Null for cluster-scoped resources.

## Import

Import existing Kubernetes resources (created by kubectl, Helm, or other tools) into Terraform management.

### Import Workflows

k8sconnect supports three import workflows:

#### 1. CLI Import (Classic)

Traditional import command - requires writing resource configuration first:

```shell
# 1. Write resource block
resource "k8sconnect_object" "nginx" {
  yaml_body = file("nginx.yaml")
  cluster_connection = var.cluster_connection
}

# 2. Run import
export KUBECONFIG=~/.kube/config
terraform import k8sconnect_object.nginx "prod:default:apps/v1/Deployment:nginx"

# 3. Verify
terraform plan  # Should show no changes
```

#### 2. Import Blocks with Config Generation (Terraform 1.5+)

**Recommended** - Terraform generates the resource configuration for you:

```hcl
# Only write the import block - no resource definition needed
import {
  to = k8sconnect_object.nginx
  id = "prod:default:apps/v1/Deployment:nginx"
}
```

```shell
# Terraform generates the full resource block
export KUBECONFIG=~/.kube/config
terraform plan -generate-config-out=generated.tf
```

This creates `generated.tf` with a complete resource block including `yaml_body` and `cluster_connection`. Review and move it to your main configuration.

**Note**: The generated `cluster_connection` will have the full kubeconfig file inlined as a string. You'll want to replace it with `kubeconfig = file("~/.kube/config")` for cleaner code.

#### 3. Import Blocks (Terraform 1.5+)

Declarative import - write both the import block and resource definition:

```hcl
import {
  to = k8sconnect_object.nginx
  id = "prod:default:apps/v1/Deployment:nginx"
}

resource "k8sconnect_object" "nginx" {
  yaml_body = file("nginx.yaml")
  cluster_connection = var.cluster_connection
}
```

```shell
export KUBECONFIG=~/.kube/config
terraform apply  # Imports and reconciles in one command
```

### How Import Works

1. **Import reads from kubeconfig**: The import process uses your `KUBECONFIG` environment variable to connect to the cluster
2. **Ownership takeover**: If the resource was created by kubectl or other tools, k8sconnect automatically takes ownership using Server-Side Apply with `force=true`
3. **Adds resource to state**: The current state is captured (with server-added fields cleaned)
4. **Configure for future operations**: After import, the `cluster_connection` in your resource definition is used for all subsequent operations

### Prerequisites

Set the `KUBECONFIG` environment variable to your kubeconfig file:

```shell
export KUBECONFIG=~/.kube/config

# Or specify a custom path
export KUBECONFIG=/path/to/your/kubeconfig
```

**Note**: The import process uses kubeconfig context names. You can view available contexts with:

```shell
kubectl config get-contexts
```

### Import ID Format

The import ID uses colons (`:`) as delimiters with the following format:

```
# Namespaced resources
context:namespace:apiVersion/Kind:name

# Cluster-scoped resources
context:apiVersion/Kind:name
```

**Components:**
- `context`: The kubeconfig context name (from `kubectl config current-context` or `kubectl config get-contexts`)
- `namespace`: The Kubernetes namespace (only for namespaced resources)
- `apiVersion/Kind`: The full API version and kind (e.g., `v1/ConfigMap`, `apps/v1/Deployment`)
- `name`: The resource name

**Why apiVersion is required**: Multiple API versions can exist for the same Kind (e.g., `autoscaling/v1` vs `autoscaling/v2` for HorizontalPodAutoscaler). The apiVersion prevents ambiguity.

### Import Examples

**Core resources (namespaced):**

```shell
terraform import k8sconnect_object.nginx "prod-cluster:default:v1/Pod:nginx-abc123"
terraform import k8sconnect_object.config "prod-cluster:default:v1/ConfigMap:app-config"
terraform import k8sconnect_object.secret "prod-cluster:kube-system:v1/Secret:tls-cert"
terraform import k8sconnect_object.svc "prod-cluster:default:v1/Service:api"
```

**Resources with API groups (namespaced):**

```shell
terraform import k8sconnect_object.app "prod-cluster:default:apps/v1/Deployment:my-app"
terraform import k8sconnect_object.sts "prod-cluster:default:apps/v1/StatefulSet:postgres"
terraform import k8sconnect_object.ing "prod-cluster:default:networking.k8s.io/v1/Ingress:api-gateway"
terraform import k8sconnect_object.hpa "prod-cluster:default:autoscaling/v2/HorizontalPodAutoscaler:app-hpa"
```

**Cluster-scoped resources:**

```shell
terraform import k8sconnect_object.namespace "prod-cluster:v1/Namespace:my-namespace"
terraform import k8sconnect_object.reader "prod-cluster:rbac.authorization.k8s.io/v1/ClusterRole:pod-reader"
terraform import k8sconnect_object.pv "prod-cluster:v1/PersistentVolume:data-vol"
terraform import k8sconnect_object.storage "prod-cluster:storage.k8s.io/v1/StorageClass:fast-ssd"
```

**Custom resources:**

```shell
terraform import k8sconnect_object.cert "prod-cluster:default:cert-manager.io/v1/Certificate:my-cert"
terraform import k8sconnect_object.app "prod-cluster:default:argoproj.io/v1alpha1/Application:my-app"
terraform import k8sconnect_object.cr "prod-cluster:default:stable.example.com/v1/MyResource:instance-1"
```

### Complete Import Workflow

**Step 1: Create the resource configuration**

Write the Terraform configuration matching your existing resource:

```terraform
resource "k8sconnect_object" "my_app" {
  yaml_body = <<-YAML
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: my-app
      namespace: default
      labels:
        app: my-app
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: my-app
      template:
        metadata:
          labels:
            app: my-app
        spec:
          containers:
          - name: nginx
            image: nginx:1.25
  YAML

  cluster_connection = {
    host                   = var.cluster_endpoint
    cluster_ca_certificate = var.cluster_ca_cert
    token                  = var.cluster_token
  }
}
```

**Step 2: Find the context name**

```shell
kubectl config current-context
# Output: prod-cluster
```

**Step 3: Find the apiVersion**

```shell
kubectl get deployment my-app -n default -o jsonpath='{.apiVersion}'
# Output: apps/v1
```

**Step 4: Run the import**

```shell
export KUBECONFIG=~/.kube/config
terraform import k8sconnect_object.my_app "prod-cluster:default:apps/v1/Deployment:my-app"
```

**Step 5: Verify the import**

```shell
# Plan should show no changes
terraform plan
```

If the plan shows changes, adjust your `yaml_body` to match the actual resource state, or use `ignore_fields` for fields managed by controllers (like HPA managing `spec.replicas`).

### Finding Import Information

**Get the context name:**

```shell
# Current context
kubectl config current-context

# List all contexts
kubectl config get-contexts
```

**Get the apiVersion:**

```shell
# For a specific resource
kubectl get <kind> <name> -n <namespace> -o jsonpath='{.apiVersion}'

# Examples
kubectl get deployment my-app -o jsonpath='{.apiVersion}'           # apps/v1
kubectl get ingress api-gateway -o jsonpath='{.apiVersion}'         # networking.k8s.io/v1
kubectl get certificate my-cert -o jsonpath='{.apiVersion}'         # cert-manager.io/v1
kubectl get namespace kube-system -o jsonpath='{.apiVersion}'       # v1
```

**Get the full resource details:**

```shell
kubectl get <kind> <name> -n <namespace> -o yaml
```

### Troubleshooting Import

**Error: "Context not found in kubeconfig"**

```
Error: Import Failed: Context Not Found
Context "my-cluster" not found in kubeconfig.
```

**Solution**: Check your context name matches exactly:

```shell
kubectl config get-contexts
# Use the exact name from the NAME column
```

**Error: "KUBECONFIG environment variable is not set"**

```
Error: Import Failed: KUBECONFIG Not Found
KUBECONFIG environment variable is not set
```

**Solution**: Set the KUBECONFIG environment variable:

```shell
export KUBECONFIG=~/.kube/config
```

**Error: "Invalid Import ID Format"**

```
Error: Invalid Import ID Format
expected 3 or 4 colon-separated parts, got X
```

**Solution**: Ensure you're using the correct format with colons:

```shell
# Correct (namespaced)
terraform import k8sconnect_object.app "context:namespace:apiVersion/Kind:name"

# Correct (cluster-scoped)
terraform import k8sconnect_object.ns "context:apiVersion/Kind:name"

# Wrong - using dots instead of colons
terraform import k8sconnect_object.app "context.namespace.Kind.name"
```

**Error: "Resource not found"**

```
Error: Import Failed: Resource Not Found
```

**Solution**: Verify the resource exists and check the namespace:

```shell
# List resources
kubectl get <kind> -n <namespace>

# Check if resource exists
kubectl get <kind> <name> -n <namespace>
```

**Plan shows changes after import**

If `terraform plan` shows changes after import, this usually means your `yaml_body` doesn't match the actual resource state:

- **Server-added fields**: Use `ignore_fields` for fields added by Kubernetes (e.g., `spec.clusterIP` for Services)
- **Controller-managed fields**: Use `ignore_fields` for fields managed by controllers (e.g., `spec.replicas` when using HPA)
- **Format differences**: Ensure your YAML formatting matches (quotes, multiline strings, etc.)
- **Default values**: Kubernetes may have added default values - include them in your config or use `ignore_fields`

**Example - Importing a Service with server-generated clusterIP:**

```terraform
resource "k8sconnect_object" "api" {
  yaml_body = <<-YAML
    apiVersion: v1
    kind: Service
    metadata:
      name: api
      namespace: default
    spec:
      type: ClusterIP
      # Don't include clusterIP - it's auto-generated
      selector:
        app: api
      ports:
      - port: 80
        targetPort: 8080
  YAML

  # Ignore server-generated fields
  ignore_fields = [
    "spec.clusterIP",
    "spec.clusterIPs",
    "spec.ports[*].nodePort"
  ]

  cluster_connection = var.cluster_connection
}
```

### Working with Imported Resources

After import, the resource is under Terraform management. Future operations will use the `cluster_connection` you configured:

```terraform
resource "k8sconnect_object" "imported_app" {
  yaml_body = file("deployment.yaml")

  # Used for plan/apply operations after import
  cluster_connection = {
    host                   = aws_eks_cluster.main.endpoint
    cluster_ca_certificate = aws_eks_cluster.main.certificate_authority[0].data
    exec = {
      api_version = "client.authentication.k8s.io/v1"
      command     = "aws"
      args        = ["eks", "get-token", "--cluster-name", "prod"]
    }
  }

  # Ignore fields managed by external controllers
  ignore_fields = ["spec.replicas"]  # If HPA manages this
}
```

**Field Ownership**: When you import a resource created by kubectl or other tools, k8sconnect will take ownership of fields in your `yaml_body`. Use `ignore_fields` to release ownership of specific fields back to controllers.
